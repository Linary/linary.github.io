<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Mac下安装HBase单机版]]></title>
    <url>%2F2019%2F01%2F22%2Fhbase%2Finstall-hbase%2F</url>
    <content type="text"><![CDATA[因项目测试需要，本人试图在Mac上安装一个单机版的HBase，但就是这么一个简单的事，却折腾了好久。主要是网上的资料都是你抄我我抄你，总是遗漏一些重要信息。在一个就是HBase自己有些坑，做的还不够友好。 废话不多说，直接写步骤。 TODO: 待补充]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS学习笔记]]></title>
    <url>%2F2019%2F01%2F11%2Ffront-end%2Fcss%2F</url>
    <content type="text"><![CDATA[CSS基础学习CSS盒模型元素分类在CSS中，html中的标签元素大体被分为三种不同的类型：块状元素、内联元素(又叫行内元素)和内联块状元素。 常用的块状元素有： 1&lt;div&gt;、&lt;p&gt;、&lt;h1&gt;...&lt;h6&gt;、&lt;ol&gt;、&lt;ul&gt;、&lt;dl&gt;、&lt;table&gt;、&lt;address&gt;、&lt;blockquote&gt; 、&lt;form&gt; 常用的内联元素有： 1&lt;a&gt;、&lt;span&gt;、&lt;br&gt;、&lt;i&gt;、&lt;em&gt;、&lt;strong&gt;、&lt;label&gt;、&lt;q&gt;、&lt;var&gt;、&lt;cite&gt;、&lt;code&gt; 常用的内联块状元素有： 1&lt;img&gt;、&lt;input&gt; 块级元素块级元素特点： 每个块级元素都从新的一行开始，并且其后的元素也另起一行。（真霸道，一个块级元素独占一行） 元素的高度、宽度、行高以及顶和底边距都可设置。 元素宽度在不设置的情况下，是它本身父容器的100%（和父元素的宽度一致），除非设定一个宽度。 设置display:block就是将元素显示为块级元素。如下代码就是将内联元素a转换为块状元素，从而使a元素具有块状元素特点。 1a&#123;display:block;&#125; 内联元素内联元素特点： 和其他元素都在一行上； 元素的高度、宽度及顶部和底部边距不可设置； 元素的宽度就是它包含的文字或图片的宽度，不可改变。 在html中，&lt;span&gt;、&lt;a&gt;、&lt;label&gt;、&lt;strong&gt;和&lt;em&gt;就是典型的内联元素（行内元素）（inline）元素。当然块状元素也可以通过代码display:inline将元素设置为内联元素。如下代码就是将块状元素div转换为内联元素，从而使 div 元素具有内联元素特点。 123456div&#123; display:inline; &#125;......&lt;div&gt;我要变成内联元素&lt;/div&gt; 内联块状元素内联块状元素（inline-block）就是同时具备内联元素、块状元素的特点，代码display:inline-block就是将元素设置为内联块状元素。(css2.1新增)，&lt;img&gt;、&lt;input&gt;标签就是这种内联块状标签。 inline-block 元素特点： 和其他元素都在一行上； 元素的高度、宽度、行高以及顶和底边距都可设置。 边框盒子模型的边框就是围绕着内容及补白的线，这条线你可以设置它的粗细、样式和颜色(边框三个属性)。 如下面代码为 div 来设置边框粗细为 2px、样式为实心的、颜色为红色的边框： div{ border:2px solid red;}上面是 border 代码的缩写形式，可以分开写： div{ border-width:2px; border-style:solid; border-color:red;} 注意： 1、border-style（边框样式）常见样式有： dashed（虚线）| dotted（点线）| solid（实线）。 2、border-color（边框颜色）中的颜色可设置为十六进制颜色，如: border-color:#888;//前面的井号不要忘掉。 3、border-width（边框宽度）中的宽度也可以设置为： thin | medium | thick（但不是很常用），最常还是用像素（px）。 现在有一个问题，如果有想为 p 标签单独设置下边框，而其它三边都不设置边框样式怎么办呢？css 样式中允许只为一个方向的边框设置样式： div{border-bottom:1px solid red;}同样可以使用下面代码实现其它三边(上、右、左)边框的设置： border-top:1px solid red;border-right:1px solid red;border-left:1px solid red; 宽度和高度盒模型宽度和高度和我们平常所说的物体的宽度和高度理解是不一样的，css内定义的宽（width）和高（height），指的是填充以里的内容范围。 因此一个元素实际宽度（盒子的宽度）=左边界+左边框+左填充+内容宽度+右填充+右边框+右边界。 元素的高度也是同理。 比如： css代码： 123456div&#123; width:200px; padding:20px; border:1px solid red; margin:10px; &#125; html代码： 123&lt;body&gt; &lt;div&gt;文本内容&lt;/div&gt;&lt;/body&gt; 元素的实际长度为：10px+1px+20px+200px+20px+1px+10px=262px。在chrome浏览器下可查看元素盒模型，如下图： 填充元素内容与边框之间是可以设置距离的，称之为“填充”。填充也可分为上、右、下、左(顺时针)。如下代码： div{padding:20px 10px 15px 30px;} 顺序一定不要搞混。可以分开写上面代码： 123456div&#123; padding-top:20px; padding-right:10px; padding-bottom:15px; padding-left:30px;&#125; 如果上、右、下、左的填充都为10px;可以这么写 1div&#123;padding:10px;&#125; 如果上下填充一样为10px，左右一样为20px，可以这么写： 1div&#123;padding:10px 20px;&#125; 边界元素与其它元素之间的距离可以使用边界（margin）来设置。边界也是可分为上、右、下、左。如下代码： 1div&#123;margin:20px 10px 15px 30px;&#125; 也可以分开写： 123456div&#123; margin-top:20px; margin-right:10px; margin-bottom:15px; margin-left:30px;&#125; 如果上右下左的边界都为10px;可以这么写： 1div&#123; margin:10px;&#125; 如果上下边界一样为10px，左右一样为20px，可以这么写： 1div&#123; margin:10px 20px;&#125; 总结一下：padding和margin的区别，padding在边框里，margin在边框外。 CSS布局模型清楚了CSS 盒模型的基本概念、 盒模型类型， 我们就可以深入探讨网页布局的基本模型了。布局模型与盒模型一样都是 CSS 最基本、 最核心的概念。但布局模型是建立在盒模型基础之上，又不同于我们常说的 CSS 布局样式或 CSS 布局模板。如果说布局模型是本，那么 CSS 布局模板就是末了，是外在的表现形式。CSS包含3种基本的布局模型，用英文概括为：Flow、Layer 和 Float。在网页中，元素有三种布局模型： 流动模型（Flow） 浮动模型 (Float) 层模型（Layer） 流动模型（Flow）先来说一说流动模型，流动（Flow）是默认的网页布局模式。也就是说网页在默认状态下的 HTML 网页元素都是根据流动模型来分布网页内容的。 流动布局模型具有2个比较典型的特征： 第一点，块状元素都会在所处的包含元素内自上而下按顺序垂直延伸分布，因为在默认状态下，块状元素的宽度都为100%。实际上，块状元素都会以行的形式占据位置； 第二点，在流动模型下，内联元素都会在所处的包含元素内从左到右水平分布显示。（内联元素可不像块状元素这么霸道独占一行） 浮动模型 (Float)块状元素这么霸道都是独占一行，如果现在我们想让两个块状元素并排显示，怎么办呢？不要着急，设置元素浮动就可以实现这一愿望。 任何元素在默认情况下是不能浮动的，但可以用 CSS 定义为浮动，如 div、p、table、img 等元素都可以被定义为浮动。如下代码可以实现两个 div 元素一行显示。 123456div&#123; width:200px; height:200px; border:2px red solid; float:left;&#125; 层模型（Layer）如何让html元素在网页中精确定位，就像图像软件PhotoShop中的图层一样可以对每个图层能够精确定位操作。CSS定义了一组定位（positioning）属性来支持层布局模型。 层模型有三种形式： 绝对定位(position: absolute) 相对定位(position: relative) 固定定位(position: fixed) 绝对定位如果想为元素设置层模型中的绝对定位，需要设置position:absolute(表示绝对定位)，这条语句的作用将元素从文档流中拖出来，然后使用left、right、top、bottom属性相对于其最接近的一个具有定位属性的父包含块进行绝对定位。如果不存在这样的包含块，则相对于body元素，即相对于浏览器窗口。 如下面代码可以实现div元素相对于浏览器窗口向右移动100px，向下移动50px。 12345678div&#123; width:200px; height:200px; border:2px red solid; position:absolute; left:100px; top:50px;&#125; 相对定位如果想为元素设置层模型中的相对定位，需要设置position:relative（表示相对定位），它通过left、right、top、bottom属性确定元素在正常文档流中的偏移位置。相对定位完成的过程是首先按static(float)方式生成一个元素(并且元素像层一样浮动了起来)，然后相对于以前的位置移动，移动的方向和幅度由left、right、top、bottom属性确定，偏移前的位置保留不动。 如下代码实现相对于以前位置向下移动50px，向右移动100px; 12345678div&#123; width:200px; height:200px; border:2px red solid; position:relative; left:100px; top:50px;&#125; 什么叫做“偏移前的位置保留不动”呢？在这个div后再添加一个span标签，会发现span元素是显示在了div元素以前位置的后面。 固定定位fixed：表示固定定位，与absolute定位类型类似，但它的相对移动的坐标是视图（屏幕内的网页窗口）本身。由于视图本身是固定的，它不会随浏览器窗口的滚动条滚动而变化，除非你在屏幕中移动浏览器窗口的屏幕位置，或改变浏览器窗口的显示大小，因此固定定位的元素会始终位于浏览器窗口内视图的某个位置，不会受文档流动影响，这与background-attachment:fixed;属性功能相同。以下代码可以实现相对于浏览器视图向右移动100px，向下移动50px。并且拖动滚动条时位置固定不变。 12345678div&#123; width:200px; height:200px; border:2px red solid; position:fixed; left:100px; top:50px;&#125; Relative与Absolute组合使用使用position:absolute可以实现被设置元素相对于浏览器（body）设置定位以后，大家有没有想过可不可以相对于其它元素进行定位呢？答案是肯定的，当然可以。使用position:relative来帮忙，但是必须遵守下面规范： 1、参照定位的元素必须是相对定位元素的前辈元素： 123&lt;div id="box1"&gt;&lt;!--参照定位的元素--&gt; &lt;div id="box2"&gt;相对参照元素进行定位&lt;/div&gt;&lt;!--相对定位元素--&gt;&lt;/div&gt; 从上面代码可以看出box1是box2的父元素（父元素当然也是前辈元素了）。 2、参照定位的元素必须加入position:relative; 12345#box1&#123; width:200px; height:200px; position:relative;&#125; 3、定位元素加入position:absolute，便可以使用top、bottom、left、right来进行偏移定位了。 12345#box2&#123; position:absolute; top:20px; left:30px; &#125; 这样box2就可以相对于父元素box1定位了（这里注意参照物就可以不是浏览器了，而可以自由设置了）。]]></content>
      <categories>
        <category>前端布局样式</category>
      </categories>
      <tags>
        <tag>CSS3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[React学习笔记]]></title>
    <url>%2F2018%2F12%2F28%2Ffront-end%2Freact%2F</url>
    <content type="text"><![CDATA[React 高级内容React 的数据视图更新原理 state 数据 JSX 模版 数据 + 模版 生成虚拟 DOM（虚拟 DOM 就是一个 JS 对象，用它来描述真实 DOM ）（损耗了性能） 用虚拟 DOM 的结构生成真实的 DOM 来显示 state 发生变化 数据 + 模版 生成新的虚拟 DOM（极大地提升了性能） 比较原始虚拟 DOM 和新的虚拟 DOM 的区别（极大地提升了性能） 直接操作 DOM，改变上一步中被修改了的的内容 优点： 性能提升 它使得跨端应用得以实现，React Native。 虚拟 DOM 中的 Diff 算法问：什么时候会需要比对？答：数据发生变化的时候，其实就是调用 setState 方法的时候 setState 方法是异步的，可以把多次 setState 统一到一起，减少 DOM 比对的次数 虚拟 DOM 是同级比对的，如果比对有差异，下面级别的就不用比对了，直接把替换下面所有的级别 虚拟 DOM 循环的时候，要给定一个唯一的 Key（最好是唯一标识 ID 之类的，不能重复的），不要使用索引作为 Key Ref 的使用 ref 可以帮助我们获取 DOM 元素 setState 是异步函数，它接收的第二个参数是 setState 完成后的回调函数 setState(() =&gt; ({}), () =&gt; {}) React 的生命周期函数生命周期函数指在某一时刻组件会自动调用执行的函数，前面讲到的 render 函数就是一个生命周期函数。 Mounting componentwillMount 函数在组件即将被挂载到页面的时刻执行，只有第一次挂载才会执行 render 可以认为就是所谓的组件挂载函数 componentDidMount 函数在组件挂载到页面之后，自动被执行，只有第一次挂载才会执行 Updation 组件被更新之前，shouldComponentUpdate(nextProps, nextState) 函数就会被自动执行。shouldComponentUpdate(nextProps, nextState) 函数返回 true 或 false。如果返回值为 false，组件和数据就不会更新，它之后的生命周期函数都不会被执行 组件被更新之前，shouldComponentUpdate 函数之后，componentWillUpdate 函数会被自动执行 组件更新完成之后，componentDidUpdate 函数会被自动执行 一个组件从父组件接受参数，只要父组件的render函数被重新执行了，子组件的 componentWillReceiveProps 就会被执行，或者用下面的两条描述 如果子组件第一次存在于父组件中，子组件的 componentWillReceiveProps 函数不会执行 如果子组件之前存在于父组件中，子组件的 componentWillReceiveProps 函数才会执行 Unmounting 当这个组件即将被从页面中剔除的时候，componentWillUnmount 函数会被自动执行 每一个组件都有这样的生命周期函数，不是只有父组件才有。 React 生命周期函数的使用场景Component 默认内置了其他所有的生命周期函数，但唯独没有内置render这个生命周期函数 借助 shouldComponentUpdate 可以避免无谓组件render函数的运行，提高性能 1234// nextProps是新的属性值shouldComponentUpdate(nextProps, nextState) &#123; return nextProps.content !== this.propscontent&#125; ajax 请求，因为只请求一次，所以不放在render()函数里面执行，建议放在componentDidMount执行；放在 componentwillMount 可能会和 rn 开发有冲突。 性能优化： 周期函数：shouldComponentUpdate(nextProps, nextState)，提高组件性能 作用域的修改：放在constructor里面。比如：this.handleClick.bind(this)，只会执行一次，避免子组件无谓渲染 react 的底层setState，内置性能提升机制，异步函数，把多次数据改变结合一一次来做，降低虚拟 DOM 比对频率 react 底层用的是虚拟DOM的概念，同层比对，还有 key 值这样的概念，提升虚拟 DOM 比对速度 使用 Charles 实现本地数据 mockCharles 可以抓取到浏览器的请求，然后返回指定数据文件的结果。 React 中实现 CSS 过渡动画12345handleToggle() &#123; this.setState(&#123; show: !this.state.show //可以这样写 &#125;) &#125; animation 的最后一个参数填写 forwards，它能够在动画结束之后保存最后一帧 css 的样式 使用 react-transition-group 实现动画安装 react 动画库 1npm install react-transition-group --save 第一次展示到页面上的时候也要动画效果，用 appear={true} Redux 入门react只是一个轻量级的视图层框架，如果要做大型应用就要搭配视图层框架redux一起使用 redux = reducer + flux，flux升级成了redux redux组件之间的传值非常简单，redux里面要求我们把数据都放在一个公共的存储区域store里面，组件之中尽量少放数据，也就是所有数据都不放在组件自身了，都把它放到一个公用的存储空间里面，然后组件改变数据就不需要传递了，改变store里面的数据之后其它的组件会感知到里面的数据发生改变。这样的话不管组件的层次有多深，但是走的流程都是一样的，会把数据的传递简化很多。 Redux 的工作流程redux是视图层框架，把所有数据都放在store之中，每个组件都要从store里拿数据，然后每个组件也要去改store里面的数据， 举例：把这个流程理解成一个图书馆的流程react compontents: 借书的人action creators: “要借什么书”这句话（语句的表达，数据的传递）store: 图书馆管理员（没办法记住所有书籍的存储情况）reducers: 图书馆管理员的记录本（要借什么书，先查有没有，要还的书查一下放到某个位置）；借书的人~我要借一本书~图书管理员听见~查阅reducers手册~去store找书~把对应的书给借书人； 创建 Redux 的 Store npm安装Redux 在store文件夹下创建index.js，import { createStore } from ‘redux’ 在store文件夹下创建reducer.js 123456const defaultState = &#123; inputValue:'123'&#125;export default (state = defaultState,action) =&gt; &#123; return state;&#125; index.js中const store = createStore(reducer); 在组件中引入，this.state = store.getState() Action 和 Reducer 的编写 react首先要改变stroe里的数据，先要派发一个action， action通过dispatch（action）方法传给store stroe 把之前的数据和action（previousState，action）传给reducer reducer是个函数，它接收了state和action以后做些处理会返回一个新的newState给到store stroe用这个新的state替换到原来的数据，stroe数据改变了以后，react组件感受到了数据变化，它会从store里面重新取数据，更新组件的内容，页面就会跟着发生变化了 Reducer 可以接收 state，但是不能修改 state。 如果常量或变量在代码里写错的时候，是会报出异常的，就可以迅速定位到问题，但是如果写一个字符串的话就不会报出异常，那样的话出了bug非常难调，所以才要进行ActionTypes的拆分。 store文件夹下创建一个actionCreators.js，把action都集中写在一个文件中，方便后期维护和自动化测试 Redux 知识点复习补充 redux三个基本原则： store必须是惟一的。 只有store能够改变自己的内容。 Reducer必须是纯函数。 纯函数：给定固定输入，就一定会有固定的输出，而且不会有任何副作用。 redux 核心 api： createStore：创建store； store.dispatch：派发action，action会传递给store。 store.getState：获取到store里面所有的数据。 store.subscribe：订阅store的改变，store改变会触发store.subscribe接受的回调函数执行。 Redux 进阶UI组件和容器组件 UI 组件负责页面的渲染（傻瓜组件） 容器组件负责页面的逻辑（聪明组件） 无状态组件当一个组件只有 render 函数时，可以用无状态组件代替。无状态组建性能比较高，没有生命周期函数。UI 组件一般可以作为无状态组件。 使用 Redux-thunk 中间件实现 ajax 数据请求redux中使用redux-thunk之后可以在action中做异步请求，action可以是一个函数，dispatch接受的action是函数的时候会自动执行这个action，action这个函数默认接受一个参数dispatch，可以用来提交action。 什么是 Redux 的中间件redux中间件是在action和store之间，对dispatch方法的封装升级。使得dispatch既可以接受对象，也可以接受函数。 如何使用 React-redux npm install react-redux –save store/index.js 引入 { createStore } from redux 引入 reducer.js const store = createStore(reducer) reducer是一个纯函数 export default (state, action) = &gt; {} todoList 组件中引入 conncect 组件连接组件和 store index.js 根组件中从react-redux 中引入 Provider const app = ( &lt; todoList /&gt;) todoList 组件通过 connect 组件把store和组件连接起来export default connect(mapStateToPropd,mapDispatchToProps)(TodoList) react-redux提供了Provider组件，用来绑定store，Provider内部的所有子组件都能够连接store。 Provider的子组件通过react-redux中的connect连接store，写法：connect(mapStateToProps, mapDispatchToProps)(Component)mapStateToProps：store中的数据映射到组件的props中；mapDispatchToProps：把store.dispatch方法挂载到props上；Component：Provider中的子组件本身； connect函数返回的是一个容器组件。 使用immutable库避免state被直接改变 immutable库提供一个fromJS方法，可以把一个JS对象转换为immutable（不可变）对象； 使用immutable.js之后，不能用“.”访问store中的对象，要使用get()方法； 使用immutable.js之后，修改store中的数据时，要使用set方法； immutable对象的set方法，会结合之前immutable对象的值和设置的值，返回一个全新的对象，并没有改变原始的state；]]></content>
      <categories>
        <category>前端 JS 框架</category>
      </categories>
      <tags>
        <tag>React</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[科学上网]]></title>
    <url>%2F2018%2F12%2F25%2Ftools%2Fscience-online%2F</url>
    <content type="text"><![CDATA[Github访问速度慢的解决方法 手动更改 hosts，更改hosts之前，你得知道修改什么网址对应的hosts。参考上面给出的链接，我主要修改的hosts地址为：github.com 和 github.global.ssl.fastly.net。查看网站对应的IP地址的方法为访问ipaddress网站，输入网址则可查阅到对应的IP地址。ipaddress地址：https://www.ipaddress.com/ 当前日期下，我查阅到的IP对应为： 12151.101.185.194 github.global.ssl.fastly.net192.30.253.113 github.com 修改的Github对应的完整hosts为： 1234567891011121314# Github192.30.253.113 github.com151.101.184.133 assets-cdn.github.com185.199.108.153 documentcloud.github.com192.30.253.118 gist.github.com185.199.108.153 help.github.com192.30.253.120 nodeload.github.com151.101.184.133 raw.github.com18.204.240.114 status.github.com192.30.253.166 training.github.com192.30.253.112 www.github.com151.101.185.194 github.global.ssl.fastly.net151.101.184.133 avatars0.githubusercontent.com151.101.184.133 avatars1.githubusercontent.com]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>上网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven常用配置]]></title>
    <url>%2F2018%2F12%2F21%2Ftools%2Fmaven%2F</url>
    <content type="text"><![CDATA[将所有依赖打成一个jar 123456789101112131415161718192021222324252627&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!--这里要替换成jar包main方法所在类 --&gt; &lt;mainClass&gt;com.baidu.hugegraph.xxx&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 拷贝依赖文件到lib 123456789101112131415161718192021222324252627282930313233343536&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;prepare-package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;overWriteReleases&gt;false&lt;/overWriteReleases&gt; &lt;overWriteSnapshots&gt;false&lt;/overWriteSnapshots&gt; &lt;overWriteIfNewer&gt;true&lt;/overWriteIfNewer&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;mainClass&gt;com.baidu.hugegraph.xxx&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell常用操作记录]]></title>
    <url>%2F2018%2F12%2F21%2Ftools%2Fshell-operation%2F</url>
    <content type="text"><![CDATA[修改 Linux 系统 Shell 的编码 123export LANG="en_US.UTF-8"export LANG="zh_CN.GBK"export LANG="zh_CN.UTF-8" 在 Java 代码里面执行 Shell 命令 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/*** 执行系统命令, 返回执行结果* @param cmd 需要执行的命令* @param dir 执行命令的子进程的工作目录, null 表示和当前主进程工作目录相同*/public static String execCmd(String cmd, File dir) &#123; StringBuilder result = new StringBuilder(); Process process = null; BufferedReader bufrIn = null; BufferedReader bufrError = null; try &#123; String[] commond = &#123;"/bin/sh", "-c", cmd&#125;; // 执行命令, 返回一个子进程对象（命令在子进程中执行） process = Runtime.getRuntime().exec(commond, null, dir); // 方法阻塞, 等待命令执行完成（成功会返回0） process.waitFor(); // 获取命令执行结果, 有两个结果: 正常的输出 和 错误的输出（PS: 子进程的输出就是主进程的输入） bufrIn = new BufferedReader(new InputStreamReader(process.getInputStream(), "UTF-8")); bufrError = new BufferedReader(new InputStreamReader(process.getErrorStream(), "UTF-8")); // 读取输出 String line = null; while ((line = bufrIn.readLine()) != null) &#123; result.append(line).append('\n'); &#125; while ((line = bufrError.readLine()) != null) &#123; result.append(line).append('\n'); &#125; &#125; catch (Exception e) &#123; logger.error(e); &#125; finally &#123; closeStream(bufrIn); closeStream(bufrError); // 销毁子进程 if (process != null) &#123; process.destroy(); &#125; &#125; // 返回执行结果 return result.toString();&#125;private static void closeStream(Closeable stream) &#123; if (stream != null) &#123; try &#123; stream.close(); &#125; catch (Exception e) &#123; // nothing &#125; &#125;&#125; Shell 中 Map 的使用 1234567891011121314151617# 定义一个空map declare -A map=()# 定义并初始化map declare -A map=(["100"]="1" ["200"]="2")# 输出所有keyecho $&#123;!map[@]&#125;# 输出所有value echo $&#123;map[@]&#125;# 添加值map["300"]="3"# 输出key对应的值echo $&#123;map["100"]&#125;# 遍历输出mapfor key in $&#123;!map[@]&#125;do echo $&#123;map[$key]&#125;done 在 Mac 环境中会出错，因为 Mac OS X 的默认Bash 是 3.x 版本，不支持 map 这种数据结构，有两种解决方案： 升级bash到 4.x 以上版本 用其他方式:比如 if elif 去到达相同的结果 tcpdump的使用 1sudo tcpdump -i lo0 dst host 127.0.0.1 and port 8080 netstat的使用 1netstat -n -p tcp | grep 8080]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度剖析 HDFS]]></title>
    <url>%2F2018%2F10%2F19%2Fhadoop%2F%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%20HDFS%2F</url>
    <content type="text"><![CDATA[本文整理自：https://www.cnblogs.com/Xmingzi/p/6032415.html 前言大数据底层技术的三大基石起源于Google在2006年发表的三篇论文：GFS、Map-Reduce、Bigtable，其中 GFS 和 Map-Reduce 直接支持了 Apache Hadoop 项目的诞生，Bigtable 催生了 NoSQL 这个崭新的数据库领域。 为弥补 Map-Reduce 处理框架高延时的缺陷，Google 在 2009 年后推出的Dremel 促使了实时计算系统的兴起，以此引发大数据第二波技术浪潮。一些大数据公司纷纷推出自己的大数据查询分析产品，如：Cloudera 开源了大数据查询分析引擎 Impala，Hortonworks 开源了 Stinger，Fackbook 开源了 Presto、UC Berkeley AMPLAB 实验室开发了 Spark 计算框架。 所有这些技术和产品的数据来源均基于 HDFS，而 HDFS 作为一个分布式文件存储系统，最基本的就是其读写操作。 目录 HDFS 名词解释 HDFS 架构 NameNode（NN） Secondary NameNode HDFS 写文件 HDFS 读文件 Block 持续化结构 HDFS 名次解释 Block：在 HDFS 中，每个文件都是采用的分块的方式存储，每个 Block 放在不同的 DataNode 上，每个 Block 的标识是一个三元组（block id， numBytes，generationStamp），其中 block id 是具有唯一性，具体分配是由 NameNode 节点设置，然后再由 DataNode 上建立 block 文件，同时建立对应 block meta 文件 Packet：在 DFSclient 与 DataNode 之间通信的过程中，发送和接受数据过程都是以一个 Packet 为基础的方式进行 Chunk：中文名字也可以称为块，但是为了与 Block 区分，还是称之为Chunk。在 DFSClient 与 DataNode 之间通信的过程中，由于文件采用的是基于块的方式来进行的，但是在发送数据的过程中是以 Packet 的方式来进行的，每个 Packet 包含了多个 Chunk，同时对于每个 Chunk 进行checksum 计算，生成 checksum bytes 小结 一个文件被拆成多个block持续化存储（block size 由配置文件参数决定） 思考： 修改 block size 对以前持续化的数据有何影响? 数据通讯过程中一个 block 被拆成 多个 packet 一个 packet 包含多个 chunk Packet结构与定义：Packet分为两类，一类是实际数据包，另一类是heatbeat包。一个Packet数据包的组成结构，如图所示 上图中，一个 Packet 是由 Header 和 Data 两部分组成，其中 Header 部分包含了一个 Packet 的概要属性信息，如下表所示： Data 部分是一个 Packet 的实际数据部分，主要包括一个 4 字节校验和（Checksum）与一个 Chunk 部分，Chunk 部分最大为 512 字节 在构建一个 Packet 的过程中，首先将字节流数据写入一个 buffer 缓冲区中，也就是从偏移量为 25 的位置（checksumStart）开始写 Packet 数据Chunk 的 Checksum 部分，从偏移量为533的位置（dataStart）开始写Packet数据的Chunk Data部分，直到一个Packet创建完成为止。 当写一个文件的最后一个 Block 的最后一个 Packet 时，如果一个 Packet 的大小未能达到最大长度，也就是上图对应的缓冲区中，Checksum 与 Chunk Data 之间还保留了一段未被写过的缓冲区位置，在发送这个Packet 之前，会检查 Chunksum 与 Chunk Data 之间的缓冲区是否为空白缓冲区（gap），如果有则将 Chunk Data 部分向前移动，使得 Chunk Data 1 与 Chunk Checksum N 相邻，然后才会被发送到 DataNode 节点。 HDFS 架构 HDFS 主要包含四类角色：Client、NameNode、SecondaryNameNode、DataNode HDFS Client：系统使用者，调用 HDFS API 操作文件，与 NameNode 交互获取文件元数据，与 DataNode 交互进行数据读写（注意：写数据时文件切分是由 Client 完成的）； NameNode：Master 节点（也称元数据节点），是系统唯一的管理者。负责元数据的管理（名称空间和数据块映射信息），配置副本策略，处理客户端请求等； DataNode：Slave 节点（也称数据存储节点），存储实际的数据，执行数据块的读写，汇报存储信息给NameNode； SecondaryNameNode：小弟角色，分担大哥 NameNode 的工作量，是NameNode 的冷备份，合并 fsimage 和 fsedits 然后再发给 NameNode，注意：在 Hadoop 2.x 版本，当启用 HDFS HA 时，将没有这一角色。 热备份：b 是 a 的热备份，如果 a 坏掉。那么 b 马上运行代替 a 的工作。冷备份：b 是 a 的冷备份，如果 a 坏掉。那么 b 不能马上代替 a 工作。但是 b 上存储 a 的一些信息，减少 a 坏掉之后的损失 HDFS 架构原则： 元数据与数据分离：文件本身的属性（即元数据）与文件所持有的数据分离； 主/从架构：一个 HDFS 集群是由一个 NameNode 和一定数目的 DataNode 组成； 一次写入多次读取：HDFS 中的文件在任何时间只能有一个 Writer。当文件被创建，接着写入数据，最后，一旦文件被关闭，就不能再修改； 移动计算比移动数据更划算：数据运算，越靠近数据，执行运算的性能就越好（数据的本地化），由于 HDFS 数据分布在不同机器上，要让网络的消耗最低，并提高系统的吞吐量，最佳方式是将运算的执行移到离它要处理的数据更近的地方，而不是移动数据。 NameNodeNameNode 是整个文件系统的管理节点，也是 HDFS 中最复杂的一个实体，它维护着 HDFS 文件系统中最重要的两个关系： HDFS 文件系统中的文件目录树，以及文件的数据块索引，即每个文件对应的数据块列表； 数据块和数据节点的对应关系，即某一个数据块保存在那些数据节点的信息； 第一个关系即目录树、元数据和数据块的索引信息会持久化到物理存储中，具体实现是保存在命名空间的镜像 fsimage 和编辑日志 edits 中。注意：在 fsimage 中，并没有记录每一个 block 对应到哪几个 DataNode 的映射信息； 第二个关系并不会持久化存储，它是在 NameNode 启动后，每个 DataNode 对本地磁盘进行扫描，将本 DataNode 上保存的 Block 信息汇报给 NameNode。NameNode 在接收到每个 DataNode 的块信息汇报后，将接收到的块信息，以及其所在的 DataNode 信息等保存在内存中。HDFS 就是通过这种块信息汇报的方式来完成 Block -&gt; DataNodes list 的映射表构建。 fsimage 记录了自最后一次检查点之前 HDFS 文件系统中所有目录和文件的序列化信息；edits 是元数据操作日志（记录每次保存 fsimage 之后到下次保存之间的所有 HDFS 操作）。 在 NameNode 启动时候，会先将 fsimage 中的文件系统元数据信息加载到内存，然后根据 eidts 中的记录将内存中的元数据同步至最新状态，然后将这个新版本的 FsImage 从内存中保存到本地磁盘上，然后删除旧的 EditLog，这个过程称为一个检查点 (checkpoint)。 类似于数据库中的检查点，为了避免 edits 日志过大，在 Hadoop 1.X 中，SecondaryNameNode 会按照时间阈值（比如24小时）或者 edits 大小阈值（比如1G），周期性的将 fsimage 和 edits 合并，然后将最新的 fsimage 推送给 NameNode。而在 Hadoop2.X 中，这个动作是由 Standby NameNode 来完成的。 由此可看出，这两个文件一旦损坏或丢失，将导致整个HDFS文件系统不可用。 在 Hadoop 1.X 为了保证这两种元数据文件的高可用性，一般的做法是将dfs.namenode.name.dir 设置成以逗号分隔的多个目录，这多个目录至少不要在一块磁盘上，最好放在不同的机器上，比如：挂载一个共享文件系统。 fsimage/edits 是序列化后的文件，想要查看或编辑里面的内容，可通过 HDFS 提供的 oiv\oev 命令，如下： 命令: hdfs oiv（offline image viewer），用于将 fsimage 文件的内容转储到指定文件中以便于阅读，如文本文件、XML文件，该命令需要以下参数： -i（必填参数）–inputFile 输入FSImage文件 -o（必填参数）–outputFile 输出转换后的文件，如果存在，则会覆盖 -p (可选参数） –processor 将FSImage文件转换成哪种格式：（Ls | XML | FileDistribution），默认为Ls 命令：hdfs oev（offline edits viewer），该工具只操作文件因而并不需要 Hadoop 集群处于运行状态。支持的输出格式有 binary（Hadoop使用的二进制格式）、XML（在不使用参数 p 时的默认输出格式）和 stats（输出 edits 文件的统计信息） 示例： 123hdfs oiv -i /data1/hadoop/dfs/name/current/fsimage_0000000000019372521 -o /home/hadoop/fsimage.txthdfs oev -i edits_0000000000000042778-0000000000000042779 -o edits.xml 小结 NameNode 管理着 DataNode，接收 DataNode 的注册、心跳、数据块提交等信息的上报，并且在心跳中发送数据块复制、删除、恢复等指令；同时，NameNode 还为客户端对文件系统目录树的操作和对文件数据读写、对 HDFS 系统进行管理提供支持。 NameNode 启动后会进入一个称为安全模式的特殊状态。处于安全模式的 NameNode 是不会进行数据块的复制的。NameNode 从所有的 DataNode 接收心跳信号和块状态报告。块状态报告包括了某个 DataNode 所有的数据 块列表。每个数据块都有一个指定的最小副本数。当 NameNode 检测确认某 个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全 (safely replicated) 的；在一定百分比（这个参数可配置）的数据块被 NameNode 检测确认是安全之后（加上一个额外的 30 秒等待时间）， NameNode 将退出安全模式状态。接下来它会确定还有哪些数据块的副本没 有达到指定数目，并将这些数据块复制到其他 DataNode 上。 Secondary NameNode在 HA cluster 中又称为 standby node。 定期合并 fsimage 和 edits 日志，将 edits 日志文件大小控制在一个限度下 NameNode 响应 Secondary NameNode 请求，将 edit log 推送给 Secondary NameNode，并且自己开始重新写一个新的 edit log Secondary NameNode 收到来自 NameNode 的 fsimage 文件和 edit log Secondary NameNode 将 fsimage 加载到内存，应用 edit log ， 并生成一个新的 fsimage 文件 Secondary NameNode 将新的 fsimage 推送给 NameNode NameNode 用新的 fsimage 取代旧的 fsimage ，在 fstime 文件中记下检查点发生的时间 HDFS 写文件 Client将FileA按64M分块。分成两块，block1和Block2; Client向nameNode发送写数据请求，如图蓝色虚线① NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线② Block1: host2,host1,host3 Block2: host7,host8,host4 client向DataNode发送block1；发送过程是以流式写入，流式写入过程如下：4.1 将64M的block1按64k的packet划分4.2 然后将第一个packet发送给host24.3 host2接收完后，将第一个packet发送给host1，同时client想host2发送第二个packet4.4 host1接收完第一个packet后，发送给host3，同时接收host2发来的第二个packet4.5 以此类推，如图红线实线所示，直到将block1发送完毕4.6 host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示4.7 client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。如图黄色粗实线4.8 发送完block1后，再向host7，host8，host4发送block2，如图蓝色实线所示 说明 当客户端向 HDFS 文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副 本系数设置为 3 ，当本地临时文件累积到一个数据块的大小时，客户端会从 Namenode 获取一个 Datanode 列表用于存放副本。然后客户端开始向第一个 Datanode 传输数据，第一个 Datanode 一小部分一小部分 (4 KB) 地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中 第二个 Datanode 节点。第二个 Datanode 也是这样，一小部分一小部分地接收数据，写入本地 仓库，并同时传给第三个 Datanode 。最后，第三个 Datanode 接收数据并存储在本地。因此， Datanode 能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个 Datanode 复制到下一个，时序图如下： 小结 写入的过程，按hdsf默认设置，1T文件，我们需要3T的存储，3T的网络流量 在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去 挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份 HDFS 读文件 客户端通过调用FileSystem对象的open()方法来打开希望读取的文件，对于HDFS来说，这个对象时分布文件系统的一个实例； DistributedFileSystem通过使用RPC来调用NameNode以确定文件起始块的位置，同一Block按照重复数会返回多个位置，这些位置按照Hadoop集群拓扑结构排序，距离客户端近的排在前面 (详见第三章） 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流，客户端对这个输入流调用read()方法 存储着文件起始块的DataNode地址的DFSInputStream随即连接距离最近的DataNode，通过对数据流反复调用read()方法，将数据从DataNode传输到客户端 到达块的末端时，DFSInputStream会关闭与该DataNode的连接，然后寻找下一个块的最佳DataNode，这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流 一旦客户端完成读取，就对FSDataInputStream调用close()方法关闭文件读取 Block 持续化结构DataNode节点上一个Block持久化到磁盘上的物理存储结构，如下图所示： 每个Block文件（如上图中blk_1084013198文件）都对应一个meta文件（如上图中blk_1084013198_10273532.meta文件），Block文件是一个一个Chunk的二进制数据（每个Chunk的大小是512字节），而meta文件是与每一个Chunk对应的Checksum数据，是序列化形式存储]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 分布式文件系统]]></title>
    <url>%2F2018%2F10%2F16%2Fhadoop%2F3-Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[前言管理网络中跨多台计算机存储的文件系统称为分布式文件系统，该系统架构于网络之上，势必会引入网络编程的复杂性。 Hadoop 有一个称为 HDFS 的分布式文件系统，但实际上，Hadoop 是一个综合性的文件系统抽象，因此它也可以集成其他文件系统，比如本地文件系统和 Amazon S3系统。 HDFS 的设计我们先来看看 HDFS 相关的一些名次，有一些是 HDFS 不支持的： 超大文件：HDFS 善于存储超大文件，比如几百 MB、几百 GB 甚至几百 TB 和 PB； 流式数据访问：适合于一次写入多次读取的场景； 商用硬件：其实就是指普通硬件，Hadoop 能够运行在普通硬件上； 低时延：HDFS 是为高数据吞吐量应用优化的，这是以提高时间延迟为代价的； 大量小文件：namenode 将文件系统的元数据存储在内存中，而无论文件大还是小，元数据都是差不多大的，所以存储 HDFS 能存储的文件数是有限制的； 多用户写，任意修改文件：HDFS 中的文件可能只有一个 writer，而且写操作总是将数据添加在文件的末尾。它不支持具有多个写入者的操作，也不允许在文件的任意位置进行修改。 HDFS 的概念数据块与普通的文件系统一样，HDFS 文件也以块为单位存储，只不过块大得多，默认为 64MB（好像较新的版本为 128MB）。但与其他文件系统不同的是，HDFS 中小于一个块大小的文件不会占据整个块的空间。 为何 HDFS 中的块如此之大？其目的是为了最小化寻址开销，如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。但是块也不能设置得太大，不然会导致 map 任务数过少，并行度不够高。 HDFS 的 fsck 指令可以显示块信息。 1hadoop fsck / -files -blocks namenode 和 datanodeHDFS 集群有两类节点以管理者-工作者模式运行，即一个 namenode 和多个 datanode。namenode 管理文件系统的命名空间，它维护者文件系统数以及整棵树内所有的文件和目录。这些信息以两个文件形式永久保存在本地磁盘上：命名空间镜像文件和编辑日志文件。namenode 中也记录着每个文件中各个块所在的数据节点信息，但它并不永久保存块的位置信息，这些信息会在系统启动时由数据节点重建。 联邦 HDFSnamenode 在内存中保存文件系统每个文件和数据块的引用关系，这意味着对于一个拥有大量文件的超大集群来说，内存将成为限制系统横向扩展的瓶颈。在 2.x 发行版中引入的联邦 HDFS 允许系统通过添加 namenode 实现扩展，其中每个 namenode 管理文件系统命名空间的一部分。比如：一个 namenode 可能管理 /user 目录下的所有文件，另一个 namenode 管理 /share 下的所有文件。 在联邦环境下，每个 namenode 维护一个命名空间卷，包括命名空间的源数据和在该命名空间下的所有文件的所有数据块的数据块池。命名空间卷之间相互独立，两两之间并不通信。 HDFS 的高可用（HA）数据高可用的前提是数据不会丢失，通过联合使用在多个文件系统中备份 namenode 的元数据和通过备用 namenode 创建监测点能防止数据丢失，但是仍然无法提供文件系统的高可用。namenode 依旧存在单点失效问题。 当 namenode 失效时，要想恢复服务，系统管理员需要启动一个拥有文件系统元数据副本的新的 namenode，并配置 datanode 和客户端以便使用这个新的 namenode。新的 namenode 直到满足以下情形才能响应服务： 将命名空间的映像导入内存中； 重做编辑日志； 接收到足够多的来自 datanode 的数据块报告并退出安全模式。 对于一个大型并拥有大量文件和数据块的集群，namenode 的冷启动需要 30 分钟甚至更长时间。 Hadoop 2.x 发行版针对上述问题在 HDFS 中增加了对高可用的支持。在这一实现中，配置了一对活动-备用 namenode。当活动 namenode 失效，备用 namenode 接会接管它的任务并开始服务于来自客户端的请求，不会有任何明显中断。实现这一目标需要在架构上做如下修改： namenode 之间需要通过高可用的共享存储实现编辑日志的共享； datanode 需要同时向两个 namenode 发送数据块处理报告； 客户端需要使用特定的机制来处理 namenode 的失效问题，这一机制应当对用户透明。 Hadoop 文件系统Hadoop 有一个抽象的文件系统概念，HDFS 只是其中的一个实现。Java 抽象类 org.apache.hadoop.fs.FileSystem定义了 Hadoop 中的一个文件系统接口，并且包含一些具体的实现，包括：Local、HDFS、HFTP、HSFTP、WebHDFS、HAR、hfs、FTP、S3等。 数据流文件读取待补充 文件写入待补充 一致模型待补充]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 MapReduce]]></title>
    <url>%2F2018%2F10%2F15%2Fhadoop%2F2-%E5%85%B3%E4%BA%8EMapReduce%2F</url>
    <content type="text"><![CDATA[使用 Hadoop 来分析数据map 和 reduceMapReduce 任务过程分为两个阶段：map 阶段和 reduce 阶段。每个阶段都以键值对作为输入和输出，其类型由程序员选择。 数据流Hadoop 在存储有输入数据（HDFS 中的数据）的节点上运行 map 任务，可以获得最佳性能，这就是所谓的“数据本地化优化”，因为这样不需要使用集群的带宽资源。但是，有时候对于一个 map 任务的输入来说，存储有某个 HDFS 数据块备份的三个节点可能正在运行其他 map 任务，此时作业调度需要在三个备份中的某个备份（数据块）寻找同个机架中空闲的机器来运行该 map 任务。仅仅在非常偶然的情况下（基本不会发生），会使用其他机架中的机器运行该 map 任务，这将导致机架与机架之间的网络传输。 这也是为什么最佳分片的大小应该与块大小相同，因为它是确保可以存储在单个节点上的最大输入块的大小。如果分片跨越两个数据块，那么对于任何一个 HDFS 节点，基本上都不可能同时存储这两个数据块，因此分片中的部分数据需要通过网络传输到 map 任务节点。与使用本地数据运行整个 map 任务相比，显然是低效的。 map 的输出一般是写入本地磁盘而非 HDFS，因为 map 的输出是中间结果，该输出需要传递给 reduce 后才产生最终输出结果，而一旦作业完成，map 的输出结果就可以删除。因此，如果把它存储在 HDFS 上，难免有些小题大做。当然如果该节点上运行的 map 任务在传送中间结果给 reduce 时出错，Hadoop 会在另一个节点上重新运行这个 map 任务以再次构建 map 中间结果。 reduce 任务并不具备数据本地化的优势，因为 reduce 的输入通常总是来自于很多 mapper 的输出。 reduce 任务的数量并非有输入数据的大小决定，而事实上是独立指定的。如果有多个 reduce 任务，每个 map 任务就会针对输出进行分区（partition），也就是为每个 reduce 任务建一个分区。分区由用户定义的 partition 函数控制，但通常用默认的 partitioner 通过哈希函数来区分就已经很高效了。 combiner 函数经过上面的介绍，我们很很容易地知道，尽量减少 map 和 reduce 之间的数据传输是有利的。Hadoop 允许我们针对 map 任务的输出指定一个 combiner 函数，该函数的输出作为 reduce 的输入。combiner 属于优化方案，不管调用 combiner 多少次，reduce 的输出结果应该都一样。 通俗地说就是能够提前合并的可以尽量通过 combiner 在 map 端就合并，比如求最值，求合这类计算，提前算和最后一起算结果都是一样的，但是注意：求平均值就不是了。 12345max(0, 20, 10, 25, 15) = 25max(max(0, 20, 10), max(25, 15)) = max(20, 25) = 25mean(0, 20, 10, 25, 15) = 14mean(mean(0, 20, 10), mean(25, 15)) = mean(10, 20) = 15]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jvm内存区域]]></title>
    <url>%2F2018%2F10%2F10%2Finterview%2FJava%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[前言常见面试题基本问题 介绍下 Java 内存区域（运行时数据区） Java 对象的创建过程（五步，建议能默写出来并且要知道每一步虚拟机做了什么） 对象的访问定位的两种方式（句柄和直接指针两种方式） 拓展问题 String 类和常量池 8 种基本类型的包装类和常量池 概述对于 Java 程序员来说，在虚拟机自动内存管理机制下，不需要像 C/C++ 程序开发程序员这样为每一个 new操作去写对应的 delete/free 操作，不容易出现内存泄漏和内存溢出问题。正是因为 Java 程序员把内存控制权利交给了 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务。 运行时的数据区域Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。 这些组成部分一些事线程私有的，其他的则是线程共享的。 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 方法区 直接内存 程序计数器程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 Java 虚拟机栈与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存(Stack)，其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。 局部变量表主要存放了编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError： 如果 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就会抛出 StackOverFlowError 异常，比如在递归调用时。 OutOfMemoryError： 如果 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出OutOfMemoryError异常。 Java 虚拟机栈也是线程私有的，每个线程都有各自的Java虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 本地方法栈和虚拟机栈所发挥的作用非常相似，区别是：虚拟机栈为虚拟机执行 Java 方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种异常。 堆Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC堆（Garbage Collected Heap）。从垃圾回收的角度，由于现在的收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代。再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。 进一步划分的目的是更好地回收内存，或者更快地分配内存。 在 JDK 1.8 中移除整个永久代，取而代之的是一个叫元空间（Metaspace）的区域（永久代使用的是JVM 的堆内存空间，而元空间使用的是物理内存，直接受到本机的物理内存限制）。 推荐阅读： 《Java 8 内存模型—永久代(PermGen)和元空间(Metaspace)》：http://www.cnblogs.com/paddix/p/5309550.html 方法区方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来的。 HotSpot 虚拟机中方法区也常被称为“永久代”，本质上两者并不等价。仅仅是因为 HotSpot 虚拟机设计团队用永久代来实现方法区而已，这样 HotSpot 虚拟机的垃圾收集器就可以像管理 Java 堆一样管理这部分内存了。但是这并不是一个好主意，因为这样更容易遇到内存溢出问题。 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 运行时常量池运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用）。 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出OutOfMemoryError 异常。 JDK 1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 推荐阅读： 《Java 中几种常量池的区分》： https://blog.csdn.net/qq_26222859/article/details/73135660 直接内存直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致OutOfMemoryError异常出现。 JDK 1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel）与缓存区（Buffer）的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据（Netty 中有大量使用）。 本机直接内存的分配不会收到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。 HotSpot 虚拟机对象探秘TODO：未完待续]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入学习Gremlin（22）：遍历终止操作]]></title>
    <url>%2F2018%2F10%2F08%2Fhugegraph%2F%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0Gremlin%EF%BC%8822%EF%BC%89%EF%BC%9A%E9%81%8D%E5%8E%86%E7%BB%88%E6%AD%A2%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[第22期 Gremlin Steps： hasNext()、next()、tryNext()、toList()、toSet()、toBulkSet()、fill()、iterate() 本系列文章的Gremlin示例均在HugeGraph图数据库上执行，环境搭建可参考准备Gremlin执行环境，本文示例均以其中的“TinkerPop关系图”为初始数据。 上一期：深入学习Gremlin（21）：待添加 说明Gremlin 中有一类特殊的操作，它能够终止遍历器的“遍历”行为，使其执行并返回结果。在这里要强调的一点：原生的 Gremlin 语句通常都是用遍历器连接起来的，但其实这些连接的过程并不会执行 Gremlin 语句，只有走到了terminalStep 时才会执行。这个模式类似于 Spark 中对RDD的map和action操作。 hasNext: 判断遍历器是否含有元素（结果），返回布尔值； next: 不传参数时获取遍历器的下一个元素，也可以传入一个整数 n，则获取后面 n 个元素； tryNext: hasNext和next的结合版，返回一个Optional对象，如果有结果还需要调用get()方法才能拿到； toList: 将所有的元素放到一个List中返回； toSet: 将所有的元素放到一个Set中返回，会去除重复元素； toBulkSet: 将所有的元素放到一个能排序的List中返回，重复元素也会保留； fill: 传入一个集合对象，将所有的元素放入该集合并返回，其实toList、toSet和toBulkSet就是通过fillStep实现的； iterate: 这个 Step 在终止操作里面有点特殊，它并不完全符合终止操作的定义。它会在内部迭代完整个遍历器但是不返回结果。 那肯定有细心的同学要问了，前面我们介绍了那么多的 Step 很多都没有加terminalStep 啊，为什么也能返回结果呢？其实这是 Tinkerpop 的 Gremlin 解析引擎对遍历器对象调用了一个IteratorUtils.asList()方法，又调用了它内部的fill()方法（注意：不是上面讲到的fill()Step）。 实例讲解下面通过实例来深入理解每一个操作。 Step hasNext() 示例1： 12// 判断顶点“linary”是否包含“created”出顶点g.V('linary').out('created').hasNext() 示例2： 12// 判断顶点“linary”是否包含“knows”出顶点g.V('linary').out('knows').hasNext() Step next() 示例1： 12// 获取顶点“javeme”的“knows”出顶点集合的下一个（第1个）g.V('javeme').out('knows').next() g.V(&#39;javeme&#39;).out(&#39;knows&#39;)返回的是一个遍历器（迭代器），每次执行这句话实际上都是获取的迭代器的第一个元素，那如果想获取第二个元素该怎么写呢？很简单，执行两次next()即可，但是这里的前提条件是遍历器中确实存在多个元素。 示例2： 1234// 获取顶点“javeme”的“knows”出顶点集合的下一个（第2个）it = g.V('javeme').out('knows')it.next()it.next() 示例3： 12// 获取顶点“javeme”的“knows”出顶点集合的前两个g.V('javeme').out('knows').next(2) next()与next(n)使用中有一点小小的区别，就是当没有元素或者没有足够多的元素时，执行next()会报错，但是执行next(n)则是返回一个空集合（List）。 Step tryNext() 示例1： 12// 试图获取顶点“javeme”的“created”出顶点集合中的下一个g.V('javeme').out('created').tryNext() 这里细心的读者会发现结果与前面概述中说的有些不同。概述中说的是返回一个Optional对象，要获取Optional对象里的值是需要调用它的get()方法的，怎么这里直接就把值给返回了呢？大家先别着急，我们再看一个例子。 示例2： 12// 试图获取顶点“javeme”的“created”入顶点集合中的下一个g.V('javeme').in('created').tryNext() 这里更加令人费解，没有“created”入顶点时竟然直接报错了，其实这是HugeGraph的实现中关于Optional的序列化所致。HugeGraph序列化Optional对象时会判断该对象内的值是否存在，如果存在则取出来序列化该值，否则填入一个null。详细代码见HugeGraphSONModule.java中关于OptionalSerializer的实现。 本文的重点在于学习Gremlin语法本身，下面给出上述两个示例的预期结果： 12Optional[v[3:HugeGraph]]Optional.empty Step toList() 示例1： 12// 获取所有“person”顶点的“created”出顶点集合，放入List中，允许包含重复结果g.V().hasLabel('person').out('created').toList() 示例2： 12// 获取所有“person”顶点的“created”入顶点集合，放入List中，允许包含重复结果g.V().hasLabel('person').in('created').toList() 结果与next(n)有些类似。 Step toSet() 示例1： 12// 获取所有“person”顶点的“created”出顶点集合，放入Set中，不允许包含重复结果g.V().hasLabel('person').out('created').toSet() 相比于toList，toSet去除了重复元素。 示例2： 12// 获取所有“person”顶点的“created”入顶点集合，放入Set中，不允许包含重复结果g.V().hasLabel('person').in('created').toSet() Step toBulkSet() 示例1： 12// 获取所有“person”顶点的“created”出顶点集合，放入BulkSet中，允许包含重复结果，排序g.V().hasLabel('person').out('created').toBulkSet() 所谓的BulkSet虽然名字上带有”Set”，但还是更像一个List，对比toList的结果，它实际上是把所有元素排了个序。 Step fill() 示例1： 1234// 创建一个List，获取所有“person”顶点的“created”出顶点，并放入该List中results = []g.V().hasLabel('person').out('created').fill(results)results Step iterate() 示例1： 123// 迭代所有“person”顶点it = g.V().hasLabel('person').iterate()it.hasNext() 调用了iterate()后遍历器内部的元素就已经全部迭代过了，所以再调用hasNext()返回false。 下一期：深入学习Gremlin（23）：待添加]]></content>
      <categories>
        <category>hugegraph</category>
      </categories>
      <tags>
        <tag>gremlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入学习Gremlin（23）：转换操作]]></title>
    <url>%2F2018%2F10%2F08%2Fhugegraph%2F%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0Gremlin%EF%BC%8823%EF%BC%89%EF%BC%9A%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[第23期 Gremlin Steps： map、flatMap() 本系列文章的Gremlin示例均在HugeGraph图数据库上执行，环境搭建可参考准备Gremlin执行环境，本文示例均以其中的“TinkerPop关系图”为初始数据。 上一期：深入学习Gremlin（22）：遍历终止操作 转换操作说明 map: 可以接受一个遍历器 Step 或 Lamda 表达式，将遍历器中的元素映射（转换）成另一个类型的某个对象（一对一），以便进行下一步处理； flatMap: 可以接受一个遍历器 Step 或 Lamda 表达式，将遍历器中的元素映射（转换）成另一个类型的某个对象流或迭代器（一对多）。 实例讲解下面通过实例来深入理解每一个操作。 Step map() 示例1： 123// 获取顶点“3:HugeGraph”的入“created”顶点的“name”属性，其实可以理解为顶点对象转化成了属性值对象g.V('3:HugeGraph').in('created').map(values('name'))// g.V('3:HugeGraph').in('created').map &#123;it.get().value('name')&#125; 自己动手将g.V(&#39;3:HugeGraph&#39;).in(&#39;created&#39;).map {it.get().value(&#39;name&#39;)}的注视打开试试效果。 示例2： 12// 先获取顶点“3:HugeGraph”的入“created”顶点，再将每个顶点转化为出边（一条）g.V('3:HugeGraph').in('created').map(outE()) 注意：顶点“javeme”其实是有三条边的，但是这里只打印出了一条。因为mapStep是一对一的转换，要想获取所有的边可以使用flatMap。 Step flatMap() 示例1： 12// 先获取顶点“3:HugeGraph”的入“created”顶点，再将每个顶点转化为出边（多条）g.V('3:HugeGraph').in('created').flatMap(outE()) 注意：这一次就能打印出顶点“javeme”的全部三条边了。 下一期：深入学习Gremlin（24）：待添加]]></content>
      <categories>
        <category>hugegraph</category>
      </categories>
      <tags>
        <tag>gremlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入学习Gremlin（18）：随机过滤与注入]]></title>
    <url>%2F2018%2F09%2F30%2Fhugegraph%2F%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0Gremlin%EF%BC%8818%EF%BC%89%EF%BC%9A%E9%9A%8F%E6%9C%BA%E8%BF%87%E6%BB%A4%E4%B8%8E%E6%B3%A8%E5%85%A5%2F</url>
    <content type="text"><![CDATA[第18期 Gremlin Steps： sample()、coin()、constant()、inject() 本系列文章的Gremlin示例均在HugeGraph图数据库上执行，环境搭建可参考准备Gremlin执行环境，本文示例均以其中的“TinkerPop关系图”为初始数据。 上一期：深入学习Gremlin（17）：待添加 随机过滤说明Gremlin支持对遍历器（traversal）上的结果进行采样或者做随机过滤。 sample: 接受一个整数值，从前一步的遍历器中采样（随机）出最多指定数目的结果； coin: 字面意思是抛硬币过滤，接受一个浮点值，该浮点值表示硬币出现正面的概率。coin Step 对前一步的遍历器中的每个元素都抛一次硬币，出现正面则可以通过，反面则被拦截。 sampleStep后能接上byStep，能以指定的属性为判断依据进行随机过滤。 注入说明Gremlin允许在遍历器中注入一些默认值或自定义值，比如在分支 Step 中给 else 路径的元素一个默认值，又或者在遍历器过程中人为地加上一些额外的元素。 constant: 通常用在choose或coalesceStep中做辅助输出，为那些不满足条件的元素提供一个默认值； inject: 能够在流（遍历器）的任何位置注入与当前遍历器同输出类型的对象，当然，也可以作为流的起始 Step 产生数据； inject只是在查询过程中添加一些额外的元素，并没有把数据真正地插入到图中 实例讲解下面通过实例来深入理解每一个操作。 Step sample() 示例1： 12// 从所有顶点的出边中随机选择2条g.V().outE().sample(2) 由于sample是随机采样，所以运行结果每次都可能不一样。另外，sample(n)表示最多采样n个，如果上一步不够n个元素自然结果是会小于n的。 示例2： 12// 从所以顶点的“name”属性中随机选取3个g.V().values('name').sample(3) 示例3： 12// 从所有的“person”中根据“age”随机选择3个g.V().hasLabel('person').sample(3).by('age') 示例4：与local联合使用做随机漫游（从某个顶点出发，随机选一条边，走到边上的邻接点；再以该点为起点，继续随机选择边，走到邻接点…） 12345// 从顶点“HugeGraph”出发做3次随机漫游g.V('3:HugeGraph') .repeat(local(bothE().sample(1).otherV())) .times(3) .path() 第1次：从“HugeGraph”沿着“Szhoney&gt;2&gt;&gt;S3:HugeGraph”走到“zhoney” 第2次：从“zhoney”沿着“Sjaveme&gt;1&gt;&gt;Szhoney”走到“javeme” 第3次：从“javeme”沿着“Sjaveme&gt;1&gt;&gt;Slinary”走到“linary” Step coin() 示例1： 12// 每个顶点按0.5的概率过滤g.V().coin(0.5) 这一次输出了5个顶点，而总共是有12个顶点的，为什么不是输出6个呢？学过概率论的应该都知道，不解释。我又多执行了两次，输出顶点数分别是5和7。 示例2： 12// 每个顶点按0.0的概率过滤g.V().coin(0.0) 示例3： 12// 每个顶点按1.0的概率过滤g.V().coin(1.0).count() 避免输出太长，加上count。 Step constant() 示例1： 1234// 输出所有“person”类顶点的“name”属性，否则输出“inhuman”（非人类）g.V().choose(hasLabel('person'), values('name'), constant('inhuman')) 示例2： 123// 与示例1功能相同，使用“coalesce”Step 实现g.V().coalesce(hasLabel('person').values('name'), constant('inhuman')) Step inject() 示例1： 12// 给顶点“HugeGraph”的作者添加一个叫“Tom”的人g.V('3:HugeGraph').in('created').values('name').inject('Tom') 示例2： 123// 在示例1的基础上计算每个元素的长度（“name”属性值的长度）g.V('3:HugeGraph').in('created').values('name').inject('Tom') .map &#123;it.get().length()&#125; 可以看到，注入的元素“Tom”与原生的元素一样参与了计算 示例3： 123// 在示例2的基础上计算走过的路径g.V('3:HugeGraph').in('created').values('name').inject('Tom') .map &#123;it.get().length()&#125;.path() 这里又能看出注入元素“Tom”与原生的元素的区别，由于“Tom”是在获取“name”属性这一步时才注入的，所以之前的从顶点“HugeGraph”出发，获取其“created”的入顶点这两步“Tom”是没有的。 示例4： 12// 使用inject创建出两个元素（顶点的id），并使用该元素作为id获取顶点及其属性“name”inject('javeme', 'linary', 'zhoney').map &#123;g.V(it.get()).next()&#125;.values('name') 12// 使用inject创建出一个“person”（顶点label），并使用该元素作为label获取顶点及其属性“name”inject('person').flatMap &#123;g.V().hasLabel(it.get())&#125;.values('name') 下一期：深入学习Gremlin（19）：待添加]]></content>
      <categories>
        <category>hugegraph</category>
      </categories>
      <tags>
        <tag>gremlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA NIO 之 Buffer]]></title>
    <url>%2F2018%2F09%2F28%2Fstudy-netty%2FBuffer%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[原文：https://segmentfault.com/a/1190000006824155 Java NIO Buffer 当我们需要与 NIO Channel 进行交互时，我们就需要使用到 NIO Buffer，即数据从 Buffer写入到 Channel 中，并且从 Channel 中读取到 Buffer 中。 实际上，NIO Buffer 其实是一块内存区域的封装，并提供了一些操作方法让我们能够方便地进行数据的读写。 Buffer 类型有： ByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 这些 Buffer 已经覆盖了能从 IO 中传输的所有的 Java 基本数据类型。 NIO Buffer 的基本使用使用 NIO Buffer 的步骤如下： 将 Channel 中的数据读取到 Buffer 中，对于 Buffer 本身处于写模式 调用 Buffer.flip() 方法，将 NIO Buffer 转换为读模式. 从 Buffer 中读取数据 调用 Buffer.clear() 或 Buffer.compact() 方法，将 Buffer 转换为写模式 当我们将数据写入到 Buffer 中时，Buffer 会记录我们已经写了多少的数据。当我们需要从 Buffer 中读取数据时，必须调用 Buffer.flip() 将 Buffer 切换为读模式。 一旦读取了所有的 Buffer 数据，那么我们必须清理 Buffer，让其变为重新可写的，清理 Buffer 可以调用 Buffer.clear() 或 Buffer.compact()。 示例： 12345678910public class Test &#123; public static void main(String[] args) &#123; IntBuffer intBuffer = IntBuffer.allocate(2); intBuffer.put(12345678); intBuffer.put(2); intBuffer.flip(); System.err.println(intBuffer.get()); System.err.println(intBuffer.get()); &#125;&#125; 上面代码中，我们分配两个单位大小的 IntBuffer，因此它可以写入两个 int 值。我们使用 put 方法将 int 值写入，然后使用 flip 方法将 buffer 转换为读模式，然后连续使用 get 方法从 buffer 中获取这两个 int 值。 每当调用一次 get 方法读取数据时，buffer 的读指针都会向前移动一个单位长度(在这里是一个 int 长度) Buffer 属性一个 Buffer 有三个属性： capacity position limit 其中 position 和 limit 的含义与 Buffer 处于读模式或写模式有关，而 capacity 的含义与 Buffer 所处的模式无关。 Capacity 一个内存块会有一个固定的大小，即容量(capacity)，我们最多写入 capacity 个单位的数据到 Buffer 中，例如一个 DoubleBuffer，其 Capacity 是 100，那么我们最多可以写入 100 个 double 数据。 Position 当从一个 Buffer 中写入数据时，我们是从 Buffer 的一个确定的位置(position)开始写入的。在最初的状态时，position 的值是 0。每当我们写入了一个单位的数据后，position 就会递增 1。 当我们从 Buffer 中读取数据时，我们也是从某个特定的位置开始读取的。当我们调用了 filp() 方法将 Buffer 从写模式转换到读模式时，position 的值会自动被设置为0。每当我们读取一个单位的数据，position 的值递增 1。 position 表示了读写操作的位置指针。 limit limit - position 表示此时还可以写入/读取多少单位的数据。例如在写模式，如果此时 limit 是 10，position 是 2，则表示已经写入了 2 个单位的数据，还可以写入 10 - 2 = 8 个单位的数据。 示例： 1234567891011121314151617public class Test &#123; public static void main(String args[]) &#123; IntBuffer intBuffer = IntBuffer.allocate(10); intBuffer.put(10); intBuffer.put(101); System.err.println("Write mode: "); System.err.println("\tCapacity: " + intBuffer.capacity()); System.err.println("\tPosition: " + intBuffer.position()); System.err.println("\tLimit: " + intBuffer.limit()); intBuffer.flip(); System.err.println("Read mode: "); System.err.println("\tCapacity: " + intBuffer.capacity()); System.err.println("\tPosition: " + intBuffer.position()); System.err.println("\tLimit: " + intBuffer.limit()); &#125;&#125; 这里我们首先写入两个 int 值，此时 capacity = 10，position = 2，limit = 10；然后我们调用 flip 转换为读模式, 此时 capacity = 10，position = 0，limit = 2。 分配 Buffer为了获取一个 Buffer 对象，我们首先需要分配内存空间。每个类型的 Buffer 都有一个 allocate() 方法，我们可以通过这个方法分配 Buffer： 1ByteBuffer buf = ByteBuffer.allocate(48); 这里我们分配了 48 * sizeof(Byte) 字节的内存空间。 1CharBuffer buf = CharBuffer.allocate(1024); 这里我们分配了大小为 1024 个字符的 Buffer，即这个 Buffer 可以存储 1024 个 Char，其大小为 1024 * 2 个字节。 Direct Buffer 和 Non-Direct Buffer 的区别Direct Buffer： 所分配的内存不在 JVM 堆上，不受 GC 的管理。（但是 Direct Buffer 的 Java 对象是由 GC 管理的，因此当发生 GC，对象被回收时，Direct Buffer 也会被释放）； 因为 Direct Buffer 不在 JVM 堆上分配，因此 Direct Buffer 对应用程序的内存占用的影响就不那么明显（实际上还是占用了这么多内存，但是 JVM 不好统计到非 JVM 管理的内存） 申请和释放 Direct Buffer 的开销比较大。因此正确的使用 Direct Buffer 的方式是在初始化时申请一个 Buffer，然后不断复用此 buffer，在程序结束后才释放此 buffer。 使用 Direct Buffer 时，当进行一些底层的系统 IO 操作时，效率会比较高，因为此时 JVM 不需要拷贝 buffer 中的内存到中间临时缓冲区中。 Non-Direct Buffer： 直接在 JVM 堆上进行内存的分配，本质上是 byte[] 数组的封装。 因为 Non-Direct Buffer 在 JVM 堆中，因此当进行操作系统底层 IO 操作中时，会将此 buffer 的内存复制到中间临时缓冲区中，因此 Non-Direct Buffer 的效率较低。 Buffer 的读写写入数据到 Buffer 123// read into buffer.int bytesRead = inChannel.read(buf); buf.put(127); 从 Buffer 中读取数据 123// read from buffer into channel.int bytesWritten = inChannel.write(buf);byte aByte = buf.get(); 重置 positionBuffer.rewind() 方法可以重置 position 的值为0，因此我们可以重新读取/写入 Buffer 了。如果是读模式，则重置的是读模式的 position，如果是写模式，则重置的是写模式的 position。 示例： 1234567891011121314151617181920212223public class Test &#123; public static void main(String[] args) &#123; IntBuffer intBuffer = IntBuffer.allocate(2); intBuffer.put(1); intBuffer.put(2); System.err.println("position: " + intBuffer.position()); intBuffer.rewind(); System.err.println("position: " + intBuffer.position()); intBuffer.put(1); intBuffer.put(2); System.err.println("position: " + intBuffer.position()); intBuffer.flip(); System.err.println("position: " + intBuffer.position()); intBuffer.get(); intBuffer.get(); System.err.println("position: " + intBuffer.position()); intBuffer.rewind(); System.err.println("position: " + intBuffer.position()); &#125;&#125; rewind() 主要针对于读模式，在读模式时，读取到 limit 后，可以调用 rewind() 方法，将读 position 置为 0。 关于 mark() 和 reset()我们可以通过调用 Buffer.mark() 将当前的 position 的值保存起来，随后可以通过调用 Buffer.reset() 方法将 position 的值恢复回来。 示例： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; IntBuffer intBuffer = IntBuffer.allocate(2); intBuffer.put(1); intBuffer.put(2); intBuffer.flip(); System.err.println(intBuffer.get()); System.err.println("position: " + intBuffer.position()); intBuffer.mark(); System.err.println(intBuffer.get()); System.err.println("position: " + intBuffer.position()); intBuffer.reset(); System.err.println("position: " + intBuffer.position()); System.err.println(intBuffer.get()); &#125;&#125; 这里我们写入两个 int 值，然后首先读取了一个值。此时读 position 的值为 1。接着我们调用 mark() 方法将当前的 position 保存起来（在读模式，因此保存的是读的 position），然后再次读取，此时 position 就是 2 了。接着使用 reset() 恢复原来的读 position，因此读 position 又为 1 了，可以再次读取数据。 flip, rewind 和 clear 的区别flip flip 方法源码 123456public final Buffer flip() &#123; limit = position; position = 0; mark = -1; return this;&#125; Buffer 的读/写模式共用一个 position 和 limit 变量，当从写模式变为读模式时，原先的 写 position 就变成了读模式的 limit。 rewind rewind 方法源码 12345public final Buffer rewind() &#123; position = 0; mark = -1; return this;&#125; rewind，即倒带，这个方法仅仅是将 position 置为 0。 clear clear 方法源码 123456public final Buffer clear() &#123; position = 0; limit = capacity; mark = -1; return this;&#125; 根据源码我们可以知道，clear 将 positin 设置为 0，将 limit 设置为 capacity。 clear 方法使用场景： 在一个已经写满数据的 buffer 中，调用 clear，可以从头读取 buffer 的数据； 为了将一个 buffer 填充满数据，可以调用 clear，然后一直写入，直到达到 limit。 示例： 123456789101112131415161718IntBuffer intBuffer = IntBuffer.allocate(2);intBuffer.flip();System.err.println("position: " + intBuffer.position());System.err.println("limit: " + intBuffer.limit());System.err.println("capacity: " + intBuffer.capacity());// 这里不能读, 因为 limit == position == 0, 没有数据.//System.err.println(intBuffer.get());intBuffer.clear();System.err.println("position: " + intBuffer.position());System.err.println("limit: " + intBuffer.limit());System.err.println("capacity: " + intBuffer.capacity());// 这里可以读取数据了, 因为 clear 后, limit == capacity == 2, position == 0,// 即使我们没有写入任何的数据到 buffer 中.System.err.println(intBuffer.get()); // 读取到0System.err.println(intBuffer.get()); // 读取到0 Buffer 的比较我们可以通过 equals() 或 compareTo() 方法比较两个 Buffer，当且仅当如下条件满足时，两个 Buffer 是相等的： 两个 Buffer 是相同类型的 两个 Buffer 的剩余的数据个数是相同的 两个 Buffer 的剩余的数据都是相同的. 通过上述条件我们可以发现，比较两个 Buffer 时，并不是 Buffer 中的每个元素都进行比较，而是比较 Buffer 中剩余的元素。]]></content>
      <categories>
        <category>网络通信</category>
      </categories>
      <tags>
        <tag>Buffer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty源码分析的调试环境准备]]></title>
    <url>%2F2018%2F09%2F27%2Fstudy-netty%2F%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87%2F</url>
    <content type="text"><![CDATA[原文：http://svip.iocoder.cn/Netty/build-debugging-environment/ 前言Netty 底层基于 JDK 的 NIO，我们为什么不直接基于 JDK 的 NIO 或者其他 NIO 框架？下面是我总结出来的原因 使用 JDK 自带的 NIO 需要了解太多的概念，编程复杂 Netty 底层 IO 模型随意切换，而这一切只需要做微小的改动 Netty 自带的拆包解包，异常检测等机制让你从 NIO 的繁重细节中脱离出来，让你只需要关心业务逻辑 Netty 解决了 JDK 的很多包括空轮训在内的 bug Netty 底层对线程，selector 做了很多细小的优化，精心设计的 reactor 线程做到非常高效的并发处理 自带各种协议栈让你处理任何一种通用协议都几乎不用亲自动手 Netty 社区活跃，遇到问题随时邮件列表或者 issue Netty 已经历各大 rpc 框架，消息中间件，分布式通信中间件线上的广泛验证，健壮性无比强大 环境依赖 JDK Git Maven IntelliJ IDEA ###]]></content>
      <categories>
        <category>网络通信</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入学习Gremlin（15）：分支运算]]></title>
    <url>%2F2018%2F09%2F25%2Fhugegraph%2F%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0Gremlin%EF%BC%8815%EF%BC%89%EF%BC%9A%E5%88%86%E6%94%AF%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[第15期 Gremlin Steps： coalesce()、optional()、union() 本系列文章的Gremlin示例均在HugeGraph图数据库上执行，环境搭建可参考准备Gremlin执行环境，本文示例均以其中的“TinkerPop关系图”为初始数据。 上一期：深入学习Gremlin（14）：待添加 分支操作说明 coalesce: 可以接受任意数量的遍历器（traversal），按顺序执行，并返回第一个能产生输出的遍历器的结果； optional: 只能接受一个遍历器（traversal），如果该遍历器能产生一个结果，则返回该结果，否则返回调用optionalStep的元素本身。当连续使用.optional()时，如果在某一步返回了调用元素本身，则后续的.optional()不会继续执行； union: 可以接受任意数量的遍历器（traversal），并能够将各个遍历器的输出合并到一起； 实例讲解下面通过实例来深入理解每一个操作。 Step coalesce() 示例1： 12345// 按优先级寻找到顶点“HugeGraph”的以下边和邻接点，找到一个就停止// 1、“implements”出边和邻接点// 2、“supports”出边和邻接点// 3、“created”入边和邻接点g.V('3:HugeGraph').coalesce(outE('implements'), outE('supports'), inE('created')).inV().path().by('name').by(label) HugeGraph这三类边都是存在的，按照优先级，返回了“implements”出边和邻接点。 示例2： 12345// 按优先级寻找到顶点“HugeGraph”的以下边和邻接点，找到一个就停止（调换了示例1中的1和2的顺序）// 1、“supports”出边和邻接点// 2、“implements”出边和邻接点// 3、“created”入边和邻接点g.V('3:HugeGraph').coalesce(outE('supports'), outE('implements'), inE('created')).inV().path().by('name').by(label) 这次由于“supports”放在了“implements”的前面，所以返回了“supports”出边和邻接点。 自己动手比较一下outE(&#39;supports&#39;), outE(&#39;implements&#39;), inE(&#39;created&#39;)在.coalesce()中随意调换顺序的区别。 Step optional() 示例1： 12// 查找顶点"linary"的“created”出顶点，如果没有就返回"linary"自己g.V('linary').optional(out('created')) 示例2： 12// 查找顶点"linary"的“knows”出顶点，如果没有就返回"linary"自己g.V('linary').optional(out('knows')) 示例3： 12// 查找每个“person”顶点的出“knows”顶点，如果存在，然后以出“knows”顶点为起点，继续寻找其出“created”顶点，最后打印路径g.V().hasLabel('person').optional(out('knows').optional(out('created'))).path() 结果中的后面四个顶点因为没有出“knows”顶点，所以在第一步返回了自身后就停止了。 Step union() 示例1： 12// 寻找顶点“linary”的出“created”顶点，邻接“knows”顶点，并将结果合并g.V('linary').union(out('created'), both('knows')).path() 示例2： 12// 寻找顶点“HugeGraph”的入“created”顶点（创作者），出“implements”和出“supports”顶点，并将结果合并g.V('3:HugeGraph').union(__.in('created'), out('implements'), out('supports'), out('contains')).path() 顶点“HugeGraph”没有“contains”边，所以只打印出了其作者（入“created”），它实现的框架（出“implements”）和支持的特性（出“supports”）。 下一期：深入学习Gremlin（16）：待添加]]></content>
      <categories>
        <category>hugegraph</category>
      </categories>
      <tags>
        <tag>gremlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入学习Gremlin（11）：统计运算]]></title>
    <url>%2F2018%2F09%2F22%2Fhugegraph%2F%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0Gremlin%EF%BC%8811%EF%BC%89%EF%BC%9A%E7%BB%9F%E8%AE%A1%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[第11期 Gremlin Steps： sum()、max()、min()、mean() 本系列文章的Gremlin示例均在HugeGraph图数据库上执行，环境搭建可参考准备Gremlin执行环境，本文示例均以其中的“TinkerPop关系图”为初始数据。 上一期：深入学习Gremlin（10）：逻辑运算 统计运算说明Gremlin可以在Number类型的流（遍历器）上做简单的统计运算，包括计算总和、最大值、最小值、均值。 下面讲解实现上述功能的具体Step： sum()：将流上的所有的数字求和； max()：对流上的所有的数字求最大值； min()：对流上的所有的数字求最小值； mean()：将流上的所有的数字求均值； 这四种Step只能作用在Number类型的流上，在java里就是继承自java.lang.Number类。 实例讲解下面通过实例来深入理解每一个操作。 Step sum() 示例1： 12// 计算所有“person”的“age”的总和g.V().hasLabel('person').values('age').sum() 示例2： 12// 计算所有“person”的“created”出边数的总和g.V().hasLabel('person').map(outE('created').count()).sum() 试着输入g.V().hasLabel(&#39;person&#39;).map(outE(&#39;created&#39;).count())看看每个“person”的“created”出边数 Step max() 示例1： 12// 计算所有“person”的“age”中的最大值g.V().hasLabel('person').values('age').max() 示例2： 12// 计算所有“person”的“created”出边数的最大值g.V().hasLabel('person').map(outE('created').count()).max() Step min() 示例1： 12// 计算所有“person”的“age”中的最小值g.V().hasLabel('person').values('age').min() 示例2： 12// 计算所有“person”的“created”出边数的最小值g.V().hasLabel('person').map(outE('created').count()).min() Step mean() 示例1： 12// 计算所有“person”的“age”的均值g.V().hasLabel('person').values('age').mean() 示例2： 12// 计算所有“person”的“created”出边数的均值g.V().hasLabel('person').map(outE('created').count()).mean() 下一期：深入学习Gremlin（12）：待添加]]></content>
      <categories>
        <category>hugegraph</category>
      </categories>
      <tags>
        <tag>gremlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入学习Gremlin（7）：查询结果排序]]></title>
    <url>%2F2018%2F09%2F21%2Fhugegraph%2F%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0Gremlin%EF%BC%887%EF%BC%89%EF%BC%9A%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[第7期 Gremlin Steps： order()、by() 本系列文章的Gremlin示例均在HugeGraph图数据库上执行，环境搭建可参考准备Gremlin执行环境，本文示例均以其中的“TinkerPop关系图”为初始数据。 上一期：深入学习Gremlin（6）：循环操作 排序说明Gremlin允许对查询的结果进行排序输出，可以指定按某个属性的升序、降序或是随机序的方式输出。排序方式可以通过单独的order()或者order().by(...)指定，而by() step又有一些变种，下面分别讲解order()和order().by(...)的用法。 1.单独使用order() Step，一般用于遍历器中的元素是属性时： order()会将结果以升序输出； order()单独使用时，必须保证遍历器（traverser）中的元素是可排序的，在 java 里就是必须实现java.lang.Comparable接口，否则会抛出异常。 2.联合使用order().by(...) Step，传入排序方式，一般用于遍历器中的元素是属性时： order().by(incr): 将结果以升序输出，这也是默认的排序方式； order().by(decr): 将结果以降序输出； order().by(shuffle): 将结果以随机序输出，每次执行结果顺序都可能不一样。 使用 order().by(…) step 但是 by() 传递的仅是一个排序方式的参数时，也必须保证遍历器（traverser）中的元素是可排序的。 3.联合使用order().by(...) Step，传入属性和排序方式，用于遍历器中的元素是顶点或边时： order().by(key): 将结果按照元素属性key的值升序排列，与order().by(key, incr)等效； order().by(key, incr): 将结果按照元素属性key的值升序排列； order().by(key, decr): 将结果按照元素属性key的值降序排列； order().by(key, shuffle): 将结果按照元素属性key的值随机序排列，每次执行结果顺序都可能不一样。 by()step不是一个真正的step，而是一个“step modulator”，与此类似的还有as()和option()step。通过by()step可以为某些step添加traversal、function、comparator等，通常的使用方式是step().by()…by()，某些step只能添加一个by()，而有一些可以添加任意数量的by()step。 实例讲解 order()，使用默认的排序（升序）输出 12// 以默认排序输出所有顶点的"name"属性值g.V().values('name').order() order().by(incr)，指定以升序输出 12// 以升序输出所有顶点的"name"属性值g.V().values('name').order().by(incr) order().by(decr)，指定以降序输出 12// 以降序输出所有顶点的"name"属性值g.V().values('name').order().by(decr) order().by(shuffle)，指定以随机序输出 12// 以随机序输出所有顶点的"name"属性值g.V().values('name').order().by(shuffle) order().by(key)，按照元素属性key的值升序（默认）排列 12// 将"person"类型的顶点按照"age"升序（默认）排列输出g.V().hasLabel('person').order().by('age') 为了使”age”属性排列显示得更清晰，我们取出顶点的”age”属性 12// 将"person"类型的顶点按照"age"升序（默认）排列，并获取"age"属性g.V().hasLabel('person').order().by('age').values('age') order().by(key, incr)，按照元素属性key的值升序排列 12// 将"person"类型的顶点按照"age"升序排列，并获取"age"属性g.V().hasLabel('person').order().by('age', incr).values('age') order().by(key, desc)，按照元素属性key的值降序排列 12// 将"person"类型的顶点按照"age"降序排列输出，并获取"age"属性g.V().hasLabel('person').order().by('age', decr).values('age') order().by(key, shuffle)，按照元素属性key的值随机序排列 12// 将"person"类型的顶点按照"age"随机序排列输出，并获取"age"属性g.V().hasLabel('person').order().by('age', shuffle).values('age') 下一期：深入学习Gremlin（8）：数据分组与去重]]></content>
      <categories>
        <category>hugegraph</category>
      </categories>
      <tags>
        <tag>gremlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入学习Gremlin（10）：逻辑运算]]></title>
    <url>%2F2018%2F09%2F21%2Fhugegraph%2F%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0Gremlin%EF%BC%8810%EF%BC%89%EF%BC%9A%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[第10期 Gremlin Steps： is()、and()、or()、not() 本系列文章的Gremlin示例均在HugeGraph图数据库上执行，环境搭建可参考准备Gremlin执行环境，本文示例均以其中的“TinkerPop关系图”为初始数据。 上一期：深入学习Gremlin（9）：条件和过滤 逻辑运算说明Gremlin支持在遍历器上加上逻辑运算进行过滤，只有满足该逻辑条件的元素才会进入下一个遍历器中。 下面讲解实现上述功能的具体Step： is()：可以接受一个对象（能判断相等）或一个判断语句（如：P.gt()、P.lt()、P.inside()等），当接受的是对象时，原遍历器中的元素必须与对象相等才会保留；当接受的是判断语句时，原遍历器中的元素满足判断才会保留，其实接受一个对象相当于P.eq()； and()：可以接受任意数量的遍历器（traversal），原遍历器中的元素，只有在每个新遍历器中都能生成至少一个输出的情况下才会保留，相当于过滤器组合的与条件； or()：可以接受任意数量的遍历器（traversal），原遍历器中的元素，只要在全部新遍历器中能生成至少一个输出的情况下就会保留，相当于过滤器组合的或条件； not()：仅能接受一个遍历器（traversal），原遍历器中的元素，在新遍历器中能生成输出时会被移除，不能生成输出时则会保留，相当于过滤器的非条件。 这四种逻辑运算Step除了像一般的Step写法以外，and()和or()还可以放在where()中以中缀符的形式出现。 实例讲解下面通过实例来深入理解每一个操作。 Step is() 示例1： 12// 筛选出顶点属性“age”等于28的属性值，与`is(P.eq(28))`等效g.V().values('age').is(28) 当没有任何一个顶点的属性“age”为28时，输出为空。 示例2： 12// 筛选出顶点属性“age”大于等于28的属性值g.V().values('age').is(gte(28)) 示例3： 12// 筛选出顶点属性“age”属于区间（27，29）的属性值g.V().values('age').is(inside(27, 29)) P.inside(a, b)是左开右开区间（a，b） 示例4： 123// 筛选出由两个或两个以上的人参与创建（“created”）的顶点// 注意：这里筛选的是顶点g.V().where(__.in('created').count().is(gt(2))).values('name') where()Step可以接受一些过滤条件，在第10期会详细介绍。 示例5： 12// 筛选出有创建者（“created”）的年龄（“age”）在20～29之间的顶点g.V().where(__.in('created').values('age').is(between(20, 29))).values('name') Step and()，逻辑与 示例1： 12// 所有包含出边“supports”的顶点的名字“name”g.V().and(outE('supports')).values('name') 示例2： 12// 所有包含出边“supports”和“implements”的顶点的名字“name”g.V().and(outE('supports'), outE('implements')).values('name') 示例3： 12// 包含边“created”并且属性“age”为28的顶点的名字“name”g.V().and(outE('created'), values('age').is(28)).values('name') 示例4：“示例3”的中缀符写法 12345// 包含边“created”并且属性“age”为28的顶点的名字“name”g.V().where(outE('created') .and() .values('age').is(28)) .values('name') Step or()，逻辑或 示例1： 12// 所有包含出边“supports”的顶点的名字“name”g.V().or(outE('supports')).values('name') 只有一个条件时，and()与or()的效果一样的。 示例2： 12// 所有包含出边“supports”或“implements”的顶点的名字“name”g.V().or(outE('supports'), outE('implements')).values('name') 注意对比与g.V().and(outE(&#39;supports&#39;), outE(&#39;implements&#39;)).values(&#39;name&#39;)的差别 示例3： 12// 包含边“created”或属性“age”为28的顶点的名字“name”g.V().or(outE('created'), values('age').is(28)).values('name') 注意对比与g.V().and(outE(&#39;created&#39;), values(&#39;age&#39;).is(28)).values(&#39;name&#39;)的差别 示例4：“示例3”的中缀符写法 12345// 包含边“created”或属性“age”为28的顶点的名字“name”g.V().where(outE('created') .or() .values('age').is(28)) .values('name') Step not()，逻辑非 示例1： 12// 筛选出所有不是“person”的顶点的“label”g.V().not(hasLabel('person')).label() 示例2： 12// 筛选出所有包含不少于两条（大于等于两条）“created”边的“person”的名字“name”g.V().hasLabel('person').not(out('created').count().is(lt(2))).values('name') 综合运用目标：获取所有最多只有一条“created”边并且年龄不等于28的“person”顶点 写法1： 12345// 与（含有小于等于一条“created”边，年龄不等于28）g.V().hasLabel('person') .and(outE('created').count().is(lte(1)), values("age").is(P.not(P.eq(28)))) .values('name') 写法2： 12345// 非（或（含有多于一条“created”边，年龄等于28））g.V().hasLabel('person') .not(or(out('created').count().is(gt(1)), values('age').is(28))) .values('name') 下一期：深入学习Gremlin（11）：待添加]]></content>
      <categories>
        <category>hugegraph</category>
      </categories>
      <tags>
        <tag>gremlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入学习Gremlin（4）：图查询返回结果数限制]]></title>
    <url>%2F2018%2F09%2F14%2Fhugegraph%2F%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0Gremlin%EF%BC%884%EF%BC%89%EF%BC%9A%E5%9B%BE%E6%9F%A5%E8%AF%A2%E8%BF%94%E5%9B%9E%E7%BB%93%E6%9E%9C%E6%95%B0%E9%99%90%E5%88%B6%2F</url>
    <content type="text"><![CDATA[第4期 Gremlin Steps： count()、range()、limit()、tail()、skip() 本系列文章的Gremlin示例均在HugeGraph图数据库上执行，环境搭建可参考准备Gremlin执行环境，本文示例均以其中的“TinkerPop关系图”为初始数据。 上一期：深入学习Gremlin（3）：has条件过滤 图查询返回结果数限制说明 Gremlin能统计查询结果集中元素的个数，且允许从结果集中做范围截取。假设某个查询操作（如：g.V()）的结果集包含8个元素，我们可以从这8个元素中截取指定部分。主要包括： count(): 统计查询结果集中元素的个数； range(m, n): 指定下界和上界的截取，左闭右开。比如range(2, 5)能获取第2个到第4个元素（0作为首个元素，上界为-1时表示剩余全部）； limit(n): 下界固定为0，指定上界的截取，等效于range(0, n)，语义是“获取前n个元素”。比如limit(3)能获取前3个元素； tail(n): 上界固定为-1，指定下界的截取，等效于range(count - n, -1)，语义是“获取后n个元素”。比如tail(2)能获取最后的2个元素； skip(n): 上界固定为-1，指定下界的截取，等效于range(n, -1)，语义是“跳过前n个元素，获取剩余的元素”。比如skip(6)能跳过前6个元素，获取最后2个元素。 实例讲解下面通过实例来深入理解每一个操作。 Step count()：查询当前traverser中的元素的个数，元素可以是顶点、边、属性、路径等。 示例1：查询图中所有顶点的个数 1g.V().count() TinkerPop关系图的圆点就是顶点，总共12个。 示例2：查询图中类型为“人person”的顶点数 1g.V().hasLabel('person').count() TinkerPop关系图的绿点就是类型为“人person”的顶点，共7个。 示例3：查询图中所有的 “人创建created” 的边数 1g.V().hasLabel('person').outE('created').count() TinkerPop关系图的所有从绿点(person)出发并且连线上为“created”的边，共8个。 示例4：查询图中所有顶点的属性数 1g.V().properties().count() TinkerPop关系图的所有顶点的属性共47个，其中“人person”共有28个，“软件software”共有19个，大家可以自己数一数。 Step range()：限定查询返回的元素的范围，上下界表示元素的偏移量，左闭右开。下界以“0”作为第一个元素，上界为“-1”时表示取到最后的元素。 示例1：不加限制地查询所有类型为“人person”的顶点 1g.V().hasLabel('person').range(0, -1) 示例2：查询类型为“人person”的顶点中的第2个到第5个 1g.V().hasLabel('person').range(2, 5) 示例3：查询类型为“人person”的顶点中的第5个到最后一个 1g.V().hasLabel('person').range(5, -1) Step limit()：查询前“n”个元素，相当于range(0, n) 示例1：查询前两个顶点 1g.V().limit(2) 示例2：查询前三条边 1g.E().limit(3) Step tail()：与limit()相反，它查询的是后“n”个元素，相当于range(count - n, -1) 示例1：查询后两个顶点 1g.V().tail(2) 示例2：查询后三条边 1g.E().tail(3) Step skip()：跳过前“n”个元素，获取剩余的全部元素 skip()在HugeGraph中（tinkerpop 3.2.5）中尚不支持，下面给出代码以及预期的结果。 12// 跳过前5个，skip(5)等价于range(5, -1)g.V().hasLabel('person').skip(5) 下一期：深入学习Gremlin（5）：查询路径path]]></content>
      <categories>
        <category>hugegraph</category>
      </categories>
      <tags>
        <tag>gremlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入学习Gremlin（0）：准备执行Gremlin的图形化环境]]></title>
    <url>%2F2018%2F09%2F07%2Fhugegraph%2F%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0Gremlin%EF%BC%880%EF%BC%89%EF%BC%9A%E5%87%86%E5%A4%87%E6%89%A7%E8%A1%8CGremlin%E7%9A%84%E5%9B%BE%E5%BD%A2%E5%8C%96%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[背景Gremlin是Apache TinkerPop框架下实现的图遍历语言，支持OLTP与OLAP，是目前图数据库领域主流的查询语言，可类比SQL语言之于关系型数据库。 HugeGraph是国内的一款开源图数据库，完全支持Gremlin语言。本文将讲述如何基于HugeGraph搭建一个执行Gremlin的图形化环境。 HugeGraph的github仓库下有很多子项目，我们这里只需要使用其中的两个：hugegraph和hugegraph-studio。 部署HugeGraphServer准备安装包方式一：源码编译打包进入hugegraph项目，克隆代码库 进入终端1$ git clone git@github.com:hugegraph/hugegraph.git 完成后会在当前目录下多出来一个hugegraph的子目录，不过这个目录里面的文件是源代码，我们需要编译打包才能生成可以运行包。 进入hugegraph目录，执行命令：12$ git checkout release-0.7$ mvn package -DskipTests 注意：一定要先切换分支，hugegraph主分支上版本已经升级到0.8.0了，但是studio似乎还没有升级，为避免踩坑我们还是使用已发布版。 经过一长串的控制台输出后，最后如果能看到BUILD SUCCESS表示打包成功。这时会在当前目录下多出来一个子目录hugegraph-0.7.4和一个压缩包hugegraph-0.7.4.tar.gz，这就是我们即将要使用可以运行的包。 本人有轻微强迫症，不喜欢源代码和二进制包放在一起，容易混淆，所以把hugegraph-0.7.4拷到上一层目录，然后删除源代码目录，这样上层目录又回归清爽了。123$ mv hugegraph-0.7.4 ../hugegraph-0.7.4$ cd ..$ rm -rf hugegraph 到这儿安装包就准备好了。不过，这样操作是需要你本地装了jdk、git和maven命令行工具的，如果你没有安装也没关系，我们还可以直接下载hugegraph官方的release包。 方法二：直接下载release包点击github代码的上面的导航releases 可以看到hugegraph目前有两个release，点击hugegraph-0.7.4.tar.gz就开始下载了。 下载完之后解压即可1$ tar -zxvf hugegraph-0.7.4.tar.gz 解压完之后能看到一个hugegraph-0.7.4目录，这个目录和用源码包打包生成的是一样的。 下面讲解如何配置参数。 配置参数虽然标题叫配置参数，但其实hugegraph的默认配置就已经能在大部分环境下直接使用了，不过还是说明一下几个重要的配置项。 进入hugegraph-0.7.4目录，修改HugeGraphServer提供服务的url (host + port)1234567891011121314$ vim conf/rest-server.properties# bind urlrestserver.url=http://127.0.0.1:8080# gremlin url to connectgremlinserver.url=http://127.0.0.1:8182# graphs list with pair NAME:CONF_PATHgraphs=[hugegraph:conf/hugegraph.properties]# authentication#auth.require_authentication=#auth.admin_token=#auth.user_tokens=[] restserver.url就是HugeGraphServer对外提供RESTful API服务的地址，host为127.0.0.1时只能在本机访问的，按需要修改其中的host和port部分即可。我这里由于studio也是准备在本地启动，8080端口也没有其他服务占用，所以不修改它。 graphs是可供连接的图名与配置项的键值对列表，hugegraph:conf/hugegraph.properties表示通过HugeGraphServer可以访问到一个名为hugegraph的图实例，该图的配置文件路径为conf/hugegraph.properties。我们可以不用去管图的配置文件，按需要修改图的名字即可。我这里仍然没有修改它。 初始化后端hugegraph启动服务之前是需要手动初始化后端的，不过大家也不要看到“手动”两个字就害怕，其实就是调一个命令的事。 1234567891011121314$ bin/init-store.shIniting HugeGraph Store...2018-09-07 16:02:12 1082 [main] [INFO ] com.baidu.hugegraph.cmd.InitStore [] - Init graph with config file: conf/hugegraph.properties2018-09-07 16:02:12 1201 [main] [INFO ] com.baidu.hugegraph.HugeGraph [] - Opening backend store &apos;rocksdb&apos; for graph &apos;hugegraph&apos;2018-09-07 16:02:12 1258 [main] [INFO ] com.baidu.hugegraph.backend.store.rocksdb.RocksDBStore [] - Opening RocksDB with data path: rocksdb-data/schema2018-09-07 16:02:12 1417 [main] [INFO ] com.baidu.hugegraph.backend.store.rocksdb.RocksDBStore [] - Failed to open RocksDB &apos;rocksdb-data/schema&apos; with database &apos;hugegraph&apos;, try to init CF later2018-09-07 16:02:12 1445 [main] [INFO ] com.baidu.hugegraph.backend.store.rocksdb.RocksDBStore [] - Opening RocksDB with data path: rocksdb-data/system2018-09-07 16:02:12 1450 [main] [INFO ] com.baidu.hugegraph.backend.store.rocksdb.RocksDBStore [] - Failed to open RocksDB &apos;rocksdb-data/system&apos; with database &apos;hugegraph&apos;, try to init CF later2018-09-07 16:02:12 1456 [main] [INFO ] com.baidu.hugegraph.backend.store.rocksdb.RocksDBStore [] - Opening RocksDB with data path: rocksdb-data/graph2018-09-07 16:02:12 1461 [main] [INFO ] com.baidu.hugegraph.backend.store.rocksdb.RocksDBStore [] - Failed to open RocksDB &apos;rocksdb-data/graph&apos; with database &apos;hugegraph&apos;, try to init CF later2018-09-07 16:02:12 1491 [main] [INFO ] com.baidu.hugegraph.backend.store.rocksdb.RocksDBStore [] - Store initialized: schema2018-09-07 16:02:12 1511 [main] [INFO ] com.baidu.hugegraph.backend.store.rocksdb.RocksDBStore [] - Store initialized: system2018-09-07 16:02:12 1543 [main] [INFO ] com.baidu.hugegraph.backend.store.rocksdb.RocksDBStore [] - Store initialized: graph2018-09-07 16:02:13 1804 [pool-3-thread-1] [INFO ] com.baidu.hugegraph.backend.Transaction [] - Clear cache on event &apos;store.init&apos; 这里可以看到，hugegraph初始化了rocksdb后端，那为什么是rocksdb而不是别的呢，其实就是上一步说的conf/hugegraph.properties中配置的。 1234567891011121314151617181920212223$ vim conf/hugegraph.properties# gremlin entrence to create graphgremlin.graph=com.baidu.hugegraph.HugeFactory# cache config#schema.cache_capacity=1048576#graph.cache_capacity=10485760#graph.cache_expire=600# schema illegal name template#schema.illegal_name_regex=\s+|~.*#vertex.default_label=vertexbackend=rocksdbserializer=binarystore=hugegraph# rocksdb backend config#rocksdb.data_path=/path/to/disk#rocksdb.wal_path=/path/to/disk... 其中backend=rocksdb就是设置后端为rocksdb的配置项。 其他的后端还包括：memory、cassandra、scylladb、hbase、mysql和palo。我们这里不用去管它，用默认的rocksdb即可。 初始化完成之后，会在当前目录下出现一个rocksdb-data的目录，这就是存放后端数据的地方，没事千万不要随意删它或移动它。 注意：初始化后端这个操作只需要在第一次启动服务前执行一次，不要每次起服务都执行。不过即使执行了也没关系，hugegraph检测到已经初始化过了会跳过。 启动服务终于到了启动服务了，同样也是一条命令 123$ bin/start-hugegraph.shStarting HugeGraphServer...Connecting to HugeGraphServer (http://127.0.0.1:8080/graphs)....OK 看到上面的OK就表示启动成功了，我们可以jps看一下进程。 12345$ jps...4101 HugeGraphServer4233 Jps... 如果还不放心，我们可以发个HTTP请求试试看。 12$ curl http://127.0.0.1:8080/graphs&#123;&quot;graphs&quot;:[&quot;hugegraph&quot;]&#125; 到这里HugeGraphServer的部署就完成了，接下来我们来部署HugeGraphStudio。 部署HugeGraphStudio步骤与部署HugeGraphServer大体类似，我们就不那么啰嗦了。 记得先返回最上层目录，避免目录嵌套在一起了。 准备安装包克隆代码库 12345678$ git clone git@github.com:hugegraph/hugegraph-studio.gitCloning into &apos;hugegraph-studio&apos;...mux_client_request_session: read from master failed: Broken piperemote: Counting objects: 326, done.remote: Compressing objects: 100% (189/189), done.remote: Total 326 (delta 115), reused 324 (delta 113), pack-reused 0Receiving objects: 100% (326/326), 1.60 MiB | 350.00 KiB/s, done.Resolving deltas: 100% (115/115), done. 编译打包 studio是一个包含前端的项目，使用react.js实现，自行打包的话需要安装npm、webpack等工具。 12$ cd hugegraph-studio$ mvn package -DskipTests studio打包的时间会稍长一点。 12345678910111213...[INFO] Reactor Summary:[INFO][INFO] hugegraph-studio ................................... SUCCESS [ 0.003 s][INFO] studio-api ......................................... SUCCESS [ 4.683 s][INFO] studio-dist ........................................ SUCCESS [01:42 min][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 01:47 min[INFO] Finished at: 2018-09-07T16:32:44+08:00[INFO] Final Memory: 34M/390M[INFO] ------------------------------------------------------------------------ 将打包好的目录拷到上一层，删除源码目录（纯个人喜好）。 123$ mv hugegraph-studio-0.7.0 ../$ cd ..$ rm -rf hugegraph-studio 至此，我的最上层目录就只剩下两个安装包，如下： 12$ lshugegraph-0.7.4 hugegraph-studio-0.7.0 配置参数进入hugegraph-studio-0.7.0目录，修改唯一的一个配置文件。 12345678910111213141516171819202122$ cd hugegraph-studio-0.7.0$ vim conf/hugegraph-studio.propertiesstudio.server.port=8088studio.server.host=localhostgraph.server.host=localhostgraph.server.port=8080graph.name=hugegraph# the directory name released by reactstudio.server.ui=ui# the file location of studio-api.warstudio.server.api.war=war/studio-api.war# default folder in your home directory, set to a non-empty value to overridedata.base_directory=~/.hugegraph-studioshow.limit.data=250show.limit.edge.total=1000show.limit.edge.increment=20# separator &apos;,&apos;gremlin.limit_suffix=[.V(),.E(),.hasLabel(STR),.hasLabel(NUM),.path()] 需要修改的参数是graph.server.host=localhost、graph.server.port=8080、graph.name=hugegraph。它们与HugeGraphServer的配置文件conf/rest-server.properties中的配置项对应，其中： graph.server.host=localhost与restserver.url=http://127.0.0.1:8080的host对应； graph.server.port=8080与的restserver.url=http://127.0.0.1:8080的port对应； graph.name=hugegraph与graphs=[hugegraph:conf/hugegraph.properties]的图名对应。 因为我之前并没有修改HugeGraphServer的配置文件conf/rest-server.properties，所以这里也不需要修改HugeGraphStudio的配置文件conf/hugegraph-studio.properties。 启动服务1$ bin/hugegraph-studio.sh studio的启动默认是不会放到后台的，所以我们会在控制台上看到一大串日志，在最底下看到如下日志表示启动成功： 12信息: Starting ProtocolHandler [http-nio-127.0.0.1-8088]16:56:24.507 [main] INFO com.baidu.hugegraph.studio.HugeGraphStudio ID: TS: - HugeGraphStudio is now running on: http://localhost:8088 然后我们按照提示，在浏览器中输入http://localhost:8088，就进入了studio的界面： 图中Gremlin下的框，就是我们输入gremlin语句进而操作hugegraph的入口了，下面我们给出一个例子。 创建关系图以下内容参考CSDN博客通过Gremlin语言构建关系图并进行图分析。 在输入框中输入以下代码以创建一个“TinkerPop关系图”： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// PropertyKeygraph.schema().propertyKey(&quot;name&quot;).asText().ifNotExist().create()graph.schema().propertyKey(&quot;age&quot;).asInt().ifNotExist().create()graph.schema().propertyKey(&quot;addr&quot;).asText().ifNotExist().create()graph.schema().propertyKey(&quot;lang&quot;).asText().ifNotExist().create()graph.schema().propertyKey(&quot;tag&quot;).asText().ifNotExist().create()graph.schema().propertyKey(&quot;weight&quot;).asFloat().ifNotExist().create()// VertexLabelgraph.schema().vertexLabel(&quot;person&quot;).properties(&quot;name&quot;, &quot;age&quot;, &quot;addr&quot;, &quot;weight&quot;).useCustomizeStringId().ifNotExist().create()graph.schema().vertexLabel(&quot;software&quot;).properties(&quot;name&quot;, &quot;lang&quot;, &quot;tag&quot;, &quot;weight&quot;).primaryKeys(&quot;name&quot;).ifNotExist().create()graph.schema().vertexLabel(&quot;language&quot;).properties(&quot;name&quot;, &quot;lang&quot;, &quot;weight&quot;).primaryKeys(&quot;name&quot;).ifNotExist().create()// EdgeLabelgraph.schema().edgeLabel(&quot;knows&quot;).sourceLabel(&quot;person&quot;).targetLabel(&quot;person&quot;).properties(&quot;weight&quot;).ifNotExist().create()graph.schema().edgeLabel(&quot;created&quot;).sourceLabel(&quot;person&quot;).targetLabel(&quot;software&quot;).properties(&quot;weight&quot;).ifNotExist().create()graph.schema().edgeLabel(&quot;contains&quot;).sourceLabel(&quot;software&quot;).targetLabel(&quot;software&quot;).properties(&quot;weight&quot;).ifNotExist().create()graph.schema().edgeLabel(&quot;define&quot;).sourceLabel(&quot;software&quot;).targetLabel(&quot;language&quot;).properties(&quot;weight&quot;).ifNotExist().create()graph.schema().edgeLabel(&quot;implements&quot;).sourceLabel(&quot;software&quot;).targetLabel(&quot;software&quot;).properties(&quot;weight&quot;).ifNotExist().create()graph.schema().edgeLabel(&quot;supports&quot;).sourceLabel(&quot;software&quot;).targetLabel(&quot;language&quot;).properties(&quot;weight&quot;).ifNotExist().create()// TinkerPopokram = graph.addVertex(T.label, &quot;person&quot;, T.id, &quot;okram&quot;, &quot;name&quot;, &quot;Marko A. Rodriguez&quot;, &quot;age&quot;, 29, &quot;addr&quot;, &quot;Santa Fe, New Mexico&quot;, &quot;weight&quot;, 1)spmallette = graph.addVertex(T.label, &quot;person&quot;, T.id, &quot;spmallette&quot;, &quot;name&quot;, &quot;Stephen Mallette&quot;, &quot;age&quot;, 0, &quot;addr&quot;, &quot;&quot;, &quot;weight&quot;, 1)tinkerpop = graph.addVertex(T.label, &quot;software&quot;, &quot;name&quot;, &quot;TinkerPop&quot;, &quot;lang&quot;, &quot;java&quot;, &quot;tag&quot;, &quot;Graph computing framework&quot;, &quot;weight&quot;, 1)tinkergraph = graph.addVertex(T.label, &quot;software&quot;, &quot;name&quot;, &quot;TinkerGraph&quot;, &quot;lang&quot;, &quot;java&quot;, &quot;tag&quot;, &quot;In-memory property graph&quot;, &quot;weight&quot;, 1)gremlin = graph.addVertex(T.label, &quot;language&quot;, &quot;name&quot;, &quot;Gremlin&quot;, &quot;lang&quot;, &quot;groovy/python/javascript&quot;, &quot;weight&quot;, 1)okram.addEdge(&quot;created&quot;, tinkerpop, &quot;weight&quot;, 1)spmallette.addEdge(&quot;created&quot;, tinkerpop, &quot;weight&quot;, 1)okram.addEdge(&quot;knows&quot;, spmallette, &quot;weight&quot;, 1)tinkerpop.addEdge(&quot;define&quot;, gremlin, &quot;weight&quot;, 1)tinkerpop.addEdge(&quot;contains&quot;, tinkergraph, &quot;weight&quot;, 1)tinkergraph.addEdge(&quot;supports&quot;, gremlin, &quot;weight&quot;, 1)// Titandalaro = graph.addVertex(T.label, &quot;person&quot;, T.id, &quot;dalaro&quot;, &quot;name&quot;, &quot;Dan LaRocque &quot;, &quot;age&quot;, 0, &quot;addr&quot;, &quot;&quot;, &quot;weight&quot;, 1)mbroecheler = graph.addVertex(T.label, &quot;person&quot;, T.id, &quot;mbroecheler&quot;, &quot;name&quot;, &quot;Matthias Broecheler&quot;, &quot;age&quot;, 29, &quot;addr&quot;, &quot;San Francisco&quot;, &quot;weight&quot;, 1)titan = graph.addVertex(T.label, &quot;software&quot;, &quot;name&quot;, &quot;Titan&quot;, &quot;lang&quot;, &quot;java&quot;, &quot;tag&quot;, &quot;Graph Database&quot;, &quot;weight&quot;, 1)dalaro.addEdge(&quot;created&quot;, titan, &quot;weight&quot;, 1)mbroecheler.addEdge(&quot;created&quot;, titan, &quot;weight&quot;, 1)okram.addEdge(&quot;created&quot;, titan, &quot;weight&quot;, 1)dalaro.addEdge(&quot;knows&quot;, mbroecheler, &quot;weight&quot;, 1)titan.addEdge(&quot;implements&quot;, tinkerpop, &quot;weight&quot;, 1)titan.addEdge(&quot;supports&quot;, gremlin, &quot;weight&quot;, 1)// HugeGraphjaveme = graph.addVertex(T.label, &quot;person&quot;, T.id, &quot;javeme&quot;, &quot;name&quot;, &quot;Jermy Li&quot;, &quot;age&quot;, 29, &quot;addr&quot;, &quot;Beijing&quot;, &quot;weight&quot;, 1)zhoney = graph.addVertex(T.label, &quot;person&quot;, T.id, &quot;zhoney&quot;, &quot;name&quot;, &quot;Zhoney Zhang&quot;, &quot;age&quot;, 29, &quot;addr&quot;, &quot;Beijing&quot;, &quot;weight&quot;, 1)linary = graph.addVertex(T.label, &quot;person&quot;, T.id, &quot;linary&quot;, &quot;name&quot;, &quot;Linary Li&quot;, &quot;age&quot;, 28, &quot;addr&quot;, &quot;Wuhan. Hubei&quot;, &quot;weight&quot;, 1)hugegraph = graph.addVertex(T.label, &quot;software&quot;, &quot;name&quot;, &quot;HugeGraph&quot;, &quot;lang&quot;, &quot;java&quot;, &quot;tag&quot;, &quot;Graph Database&quot;, &quot;weight&quot;, 1)javeme.addEdge(&quot;created&quot;, hugegraph, &quot;weight&quot;, 1)zhoney.addEdge(&quot;created&quot;, hugegraph, &quot;weight&quot;, 1)linary.addEdge(&quot;created&quot;, hugegraph, &quot;weight&quot;, 1)javeme.addEdge(&quot;knows&quot;, zhoney, &quot;weight&quot;, 1)javeme.addEdge(&quot;knows&quot;, linary, &quot;weight&quot;, 1)hugegraph.addEdge(&quot;implements&quot;, tinkerpop, &quot;weight&quot;, 1)hugegraph.addEdge(&quot;supports&quot;, gremlin, &quot;weight&quot;, 1) 点击右上角的三角按钮，这样就创建出了一个图。 图查询在输入框中输入： 1g.V() 就能查出上面创建的图的所有顶点和边。 至此，执行Gremlin的图形化环境就已经搭建完成，后续就可以做各种各样炫酷的gremlin查询了。]]></content>
      <categories>
        <category>hugegraph</category>
      </categories>
      <tags>
        <tag>gremlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty源码剖析]]></title>
    <url>%2F2018%2F09%2F06%2Fnetty%2F</url>
    <content type="text"><![CDATA[Thread -&gt; NioEventLoop, Netty的发送机，主要包含两种线程，一种处理连接，一种发送接收数据。 while(true) -&gt; run() Socket -&gt; Channel IOBytes -&gt; ByteBuf // 就是一个供读写的数据链Logic Chain -&gt; Pipeline Logic -&gt; ChannelHandler Question: 1、socket在哪里创建 Netty服务端启动1、创建服务端Channel bind -&gt; initAndRegister -&gt; newChannel channelFactory.newChannel()通过反射创建Channel 1、newSocket通过JDK创建底层channel 2、NioServerScoektSocketChannelCOnfig（TCP参数配置） 3、AbstractNioChannel.configureBlocking(false) channelFactory 的clazz是在bootStrap的.channel(Class)传入的 2、初始化服务端Channel init 1、init() 2、setchannelOptions, ChannelAttrs 3、setChildOptions、ChildAttrs 4、config Handler 5、add ServerBootstrapAcceptor(一个特殊的Handler) 3、注册Selector 1、AbstractChannel.register(channel) channelAddedchannelRegistered 4、端口绑定 1、AbstractUnsafe.bind() 服务端启动核心路径总结 newChannel() -&gt; init() -&gt; register() -&gt; doBind() NioEventLoop三个问题： 默认Netty服务端起多少个线程？何时启动 Netty是如何解决jdk空轮询bug的 Netty如何保 NioEventLoop的创建new NioEventLoopGroup() NioEventLoop的启动NioEventLoop的执行逻辑]]></content>
      <categories>
        <category>网络通信</category>
      </categories>
      <tags>
        <tag>Netty，NIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发与高并发]]></title>
    <url>%2F2018%2F08%2F30%2Fconcurrent%2F</url>
    <content type="text"><![CDATA[并发与高并发的关注点 并发：多个线程操作相同的资源，保证程序线程安全，合理使用资源 高并发：服务能同时处理很多请求，提高程序性能 基础知识CPU多级缓存 为什么需要CPU缓存： 缓存存在的意义：时间一致性，空间一致性 缓存一致性（MESI）：用于保证多个CPU缓存之间缓存共享数据的一致性（没听懂） 乱序执行优化：处理器为提高运算速度而做出违背代码原有顺序的优化 J.U.C 之 AQS 组件CountDownLatch计数器，能阻塞（await）某些线程直到 CountDownLatch 的值变为 0，其他线程负责将 CountDownLatch 减 1（countDown）。 计数器只能使用一次。 Semaphore信号量，能控制一定数量线程的并发执行。 在执行业务代码前，调用acquire()获取一个许可，执行完之后，调用release()释放一个许可。 还允许尝试获取许可，尝试获取多个许可以及它们的超时版。 CyclicBarrier允许多个线程互相等待，到达全部准备好的状态。就像赛跑时所有人都要在起跑线上准备好，然后再一起开跑。 可以重置。 synchronized可重入锁，一个线程进去过一次后还可以再进去一次。 其实 synchronized 也是可重入锁。 自从 synchronized 引入了偏向锁，自旋锁之后，性能与 synchronized 差不多了。 StampedLockStampedLock 是 Java8 引入的一种新的所机制，简单的理解，可以认为它是读写锁的一个改进版本。读写锁虽然分离了读和写的功能，使得读与读之间可以完全并发，但是读和写之间依然是冲突的，读锁会完全阻塞写锁，它使用的依然是悲观的锁策略。如果有大量的读线程，他也有可能引起写线程的饥饿。而StampedLock 则提供了一种乐观的读策略，这种乐观策略的锁非常类似于无锁的操作，使得乐观锁完全不会阻塞写线程。 Fork/Join 框架并行流就是把一个内容分成多个数据块，并用不同的线程分别处理每个数据块的流。并行流的底层其实就是ForkJoin框架的一个实现。 Fork/Join框架：在必要的情况下，将一个大任务，进行拆分（fork） 成若干个子任务（拆到不能再拆，这里就是指我们制定的拆分的临界值），再将一个个小任务的结果进行join汇总。 Fork/Join采用“工作窃取模式”，当执行新的任务时他可以将其拆分成更小的任务执行，并将小任务加到线程队列中，然后再从一个随即线程中偷一个并把它加入自己的队列中。 就比如两个CPU上有不同的任务，这时候A已经执行完，B还有任务等待执行，这时候A就会将B队尾的任务偷过来，加入自己的队列中，对于传统的线程，ForkJoin更有效的利用的CPU资源！ BlockingQueueBlockingQueue 通常用于一个线程生产对象，而另外一个线程消费这些对象的场景。一个线程往里边放，另外一个线程从里边取。一个线程将会持续生产新对象并将其插入到队列之中，直到队列达到它所能容纳的临界点。也就是说，它是有限的。如果该阻塞队列到达了其临界点，负责生产的线程将会在往里边插入新对象时发生阻塞。它会一直处于阻塞之中，直到负责消费的线程从队列中拿走一个对象。负责消费的线程将会一直从该阻塞队列中拿出对象。如果消费线程尝试去从一个空的队列中提取对象的话，这个消费线程将会处于阻塞之中，直到一个生产线程把一个对象丢进队列。 直接提交队列：SynchronousQueue，没有容量，所以提交的任务不能保存，总是将任务交给空闲线程,如果没有空闲线程，就创建线程，一旦达到maximumPoolSize就执行拒绝策略 有界任务队列：ArrayBlockingQueue,当线程池的数量小于corePoolSize时，当有新的任务时，创建线程，达到corePoolSize后，则将任务存到ArrayBlockingQueue中，直到有界队列容量已满时，才可能会将线程数提升到corePoolSize之上。 无界队列：LinkedBlockingQueue,除非系统资源耗尽，否则不存在任务队列入队失败的情况，因此当线程数达到corePoolSize之后，就不会增加，有新的任务到来时，都会放到无界队列中。 优先任务队列：PriorityBlockingQueue是带有优先级的队列，特殊的无界队列,理论上来说不是先入先出的，是根据任务的优先级来确定执行顺序 DelayQueue：执行定时任务，将任务按延迟时间长短放入队列中，延迟时间最短的最先被执行，存放在队列头部的是延迟期满后保存时间最长的任务 LinkedTransferQueue：其实和SynchronousQueue类似，当生产者生产出产品后，当先去找是否有消费者，如果有消费者在等待资源，则直接调用transfer()方法将资源给消费者消费，而不会放入队列中。如果没有消费者等待，则当生产者调用transfer()方法时会阻塞，而调用其他的方法，如aput()则不会阻塞，会把资源放到队列中，因为put()方法只有在队列满的时候才会阻塞。适用于游戏服务器中，可以是并发时消息传递的效率更高 多线程并发最佳实践 使用本地变量 使用不可变类 最小化锁的作用域范围：S=1/(1-a + a/n) 使用线程池的 Executor，而不是直接 new Thread 执行 宁可使用同步也不要使用线程 wait 和 notify 使用 BlockingQueue 实现生产消费模式 使用并发集合而不是加了锁的同步集合 使用 Semaphore 创建有界的访问 宁可使用同步代码块，也不使用同步的方法 避免使用静态变量 高并发处理的思路及手段 扩容：水平扩容、垂直扩容 缓存：Redis、Memcache、Guava Cache等的介绍与使用 队列：kafka、RabbitMQ、RocketMQ等 应用拆分：服务化Dubbo与微服务Spring Cloud 限流：Guava RateLimiter，常用限流算法 服务降级与服务熔断：Hystrix 数据库切库、分库、分表 高可用：任务调度分布式elastic-job、主备curator的实现、监控报警机制 缓存浏览器 -&gt; 网络转发 -&gt; 服务 -&gt; 数据库 其实缓存可以出现在上述的各个环节中。 特征 命中率：命中数 /（命中数 + 未命中数） 最大元素（空间） 清空策略：FIFO、LFU、LRU、过期时间、随机等 缓存命中率影响因素 业务场景和业务需求 缓存的设计（粒度和策略） 缓存容量和基础设施 缓存分类和应用场景 本地缓存：编程实现（成员变量、局部变量、静态变量）、Guava Cache 分布式缓存：MemCache、Redis Guava Cache灵感来源于ConcurrentHashMap。 MemCache客户端：采用一致性哈希算法，把某个key的操作映射到固定机器上。 Redis远程内存数据库，支持数据持久化。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于JDK命令行工具的监控]]></title>
    <url>%2F2018%2F08%2F22%2FJVM-0%2F</url>
    <content type="text"><![CDATA[JVM 参数类型标准参数X参数 非标准化参数 -Xint：解释执行 -Xcomp：第一使用就编译成本地代码 -Xmixed：混合模式，JVM自己来决定是否编译成本地代码 XX参数 Boolean类型 格式：-XX:[+-]表示启动或禁用name属性 非Boolean类型 格式：-XX:=表示name属性的值是value 需要注意的是：-Xms和-Xmx虽然是以X打头，但是实际上是XX参数 -Xms等价于-XX:InitialHeapSize -Xmx等价于-XX:MaxHeapSize 查看JVM运行时的参数 -XX:+PrintFlagsInitial -XX:+PrintFlagsFinal -XX:+UnlockExpire =表示默认值，:=表示用户修改过后的值 1jinfo -flag MaxHeapSize &#123;pid&#125; jstat查看JVM统计信息类加载 -class 垃圾收集 -gc JIT编译 -compiler JVM的内存结构堆区： Young: S0 + S1 + Eden Old 非堆区（Metaspace）： CCS CodeCache jamp + MAT 实战内存溢出Java里面的内存溢出是说创建的对象一直不释放 堆内存溢出 构造一个List不停地添加普通对象 -Xmx32M, -Xms32M 非堆内存溢出 构造一个List不停地添加Class对象 -XX:MetaspaceSize=32M, -XX:MaxMetaspaceSize=32M 如何导出内存映像文件 内存溢出自动导出： 使用jmap命令手动导出：jamp -demp:format=b.file=heap.hprof MAT分析内存溢出官网下载，将上一步的文件导入。 jstack实战死循环与死锁监控远程普通Java进程远程启动 Java 进程时加上如下的参数，然后通过 JvisualM 就可以通过JMX连接进行监控了。 nohup java -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port={port}-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false-Djava.net.preferIPv4Stack=true -Djava.rmi.server.hostname={ip} -jar {jar_name}.jar &amp; Btrace（可以考虑基于Btrace实现一个监控工具）Btrace 本质上就是一个拦截器，可以给 Java 进程动态添加拦截，它不需要修改原有的 Java 代码，通过一个独立进程提供监控。 默认只能在本地运行 生产环境下可以使用，但是被修改的字节码不会被还原 1btrace pid &#123;Btrace脚本&#125; 拦截方法 拦截时机 拦截this、参数、返回值 其他 拦截方法]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HBase学习]]></title>
    <url>%2F2018%2F08%2F20%2Fstudy-hbase%2F</url>
    <content type="text"><![CDATA[HBase中为什么要有Column FamilyHBase本身的设计目标是支持稀疏表，而稀疏表通常会有很多列，但是每一行有值的列又比较少。如果不使用Column Family的概念，那么有两种设计方案： 1.把所有列的数据放在一个文件中（也就是传统的按行存储）。那么当我们想要访问少数几个列的数据时，需要遍历每一行，读取整个表的数据，这样子是很低效的。 2.把每个列的数据单独分开存在一个文件中（按列存储）。那么当我们想要访问少数几个列的数据时，只需要读取对应的文件，不用读取整个表的数据，读取效率很高。然而，由于稀疏表通常会有很多列，这会导致文件数量特别多，这本身会影响文件系统的效率。 而Column Family的提出就是为了在上面两种方案中做一个折中。HBase中将一个Column Family中的列存在一起，而不同Column Family的数据则分开。由于在HBase中Column Family的数量通常很小，同时HBase建议把经常一起访问的比较类似的列放在同一个Column Family中，这样就可以在访问少数几个列时，只读取尽量少的数据。]]></content>
      <categories>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue入门]]></title>
    <url>%2F2018%2F08%2F19%2Ffront-end%2Fstudy-vue%2F</url>
    <content type="text"><![CDATA[Vue 生命周期 动画 可以使用animate.css这个库添加很多炫酷的动画； 使用velocity.js库也可以添加动画（依靠js的钩子实现的动画）； 其他注意要点 子组件的data必须是一个函数 子组件不应该修改父组件传递进来的值]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>Vue.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存数据库事务]]></title>
    <url>%2F2018%2F08%2F06%2Fmemory-db%2F</url>
    <content type="text"><![CDATA[前言 我们在使用数据库的时候，应该都体验过事务，我们对事务最直观的感受就是：一系列的操作要么全部生效，要么全部不生效，不会最后处于一种中间状态。其实这句话似乎只能体现事务的原子性，那其他几个特性呢？本文会先回顾一下事务的定义和ACID特性，再以一个具体的例子展示如何实现事务的四个特性。 事务定义所谓事务，它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。 特性 原子性（Atomicity）：事务是一个不可再分割的工作单位，事务中的操作要么都发生，要么都不发生； 一致性（Consistency）：事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。这是说数据库事务不能破坏关系数据的完整性以及业务逻辑上的一致性。 隔离性（Isolation）：多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务运行效果。 持久性（Durability）：事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式存储系统]]></title>
    <url>%2F2018%2F08%2F06%2Fdistributed-storage%2F</url>
    <content type="text"><![CDATA[需要调研的分布式存储系统包括但不限于： 谷歌的GFS、BigTable、MegaStore和Spanner，阿里的TFS、Tair和OceanBase，Facebook的Haystack，亚马逊的Dynamo，Oracle的Mysql Sharding，PingCap 的TiDB等。]]></content>
      <categories>
        <category>分布式存储系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自己实现一个轻量级的任务（线程）管理器]]></title>
    <url>%2F2018%2F07%2F23%2Fepoch-taskmanager%2F</url>
    <content type="text"><![CDATA[功能 允许自定义任务（继承任务基类），比如实时任务，延时任务，周期任务等，实时任务和延时任务都是执行一次，周期任务会反复执行。 允许定义任务链，依次顺序执行，上游任务失败了下游任务不会执行。但是不提供下游任务失败了上游任务回滚的能力。 允许提交，暂停（非必须），继续，重做和取消任务。 允许根据多种方式查询任务，比如查询根据任务Id查询，根据条件查询等，获取任务状态，进度等信息。 允许自定义任务的回调函数（成功、取消，超时，失败的响应）。 支持任务的持久化（非必须）。 关键点1、什么时候保存任务的信息？ 任务第一次提交给任务管理器后，保存。 任务成功或失败后，保存。 2、查询任务信息是每次都从后端查吗？ 任务创建后即加入到内存的容器中，在任务完成前都是从内存中查询，任务成功或者失败后（保存到数据库），从内存中删除任务的信息，而后的查询是从数据库查的。 3、当TaskManager退出时，应该扫描全部的任务，将内存中所有的任务（状态应该都是未完成）的快照都保存到数据库。以便于下次继续执行任务。 类结构设计1、Task 保存任务的静态信息，包括执行体（Callable），名称，类型，描述，参数，超时时间（可不设置）。 2、TaskTracker 保存任务的动态信息，包括进度，状态，运行时间等。]]></content>
      <categories>
        <category>调度管理</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Guava Futures异步回调机制源码解析]]></title>
    <url>%2F2018%2F07%2F23%2Fguava-future%2F</url>
    <content type="text"><![CDATA[前言 最近本人在实现一个异步任务调度框架，不打算依赖于任何第三方包。在实现任务状态监听时遇到了一些困惑，于是想了解一下Guava中的ListenableFuture的实现方式。ListenableFuture实现非阻塞的方式是其提供了回调机制(机制)，下面将阐述该回调机制的实现，主要对Futures的addCallback方法源码进行剖析。 Guava Futures简介Google Guava框架的 com.google.common.util.concurrent包是并发相关的包，它是对JDK自带concurrent包中Future和线程池相关类的扩展，从而衍生出一些新类，并提供了更为广泛的功能。在项目中常用的该包中类如下所示： ListenableFuture：该接口扩展了Future接口，增加了addListener方法，该方法在给定的excutor上注册一个监听器，当计算完成时会马上调用该监听器。不能够确保监听器执行的顺序，但可以在计算完成时确保马上被调用。 FutureCallback：该接口提供了OnSuccess和onFailure方法。获取异步计算的结果并回调。 MoreExecutors：该类是final类型的工具类，提供了很多静态方法。例如listeningDecorator方法初始化ListeningExecutorService方法，使用此实例submit方法即可初始化ListenableFuture对象。 ListenableFutureTask：该类是一个适配器，可以将其它Future适配成ListenableFuture。 ListeningExecutorService：该类是对ExecutorService的扩展，重写了ExecutorService类中的submit方法，并返回ListenableFuture对象。 JdkFutureAdapters：该类扩展了FutureTask类并实现ListenableFuture接口，增加了addListener方法。 Futures：该类提供和很多实用的静态方法以供使用。 Futures.addCallback方法源码剖析下面将模拟异步发送请求，并对请求结果进行(回调)监听。这里使用Spring框架提供的AsyncRestTemplate，来发送http请求，并获取一个org.springframework.util.concurrent.ListenableFuture对象，此时的对象是spring框架中的ListenableFuture对象。由于org.springframework.util.concurrent包中只提供了最基本的监听功能，没有其它额外功能，这里将其转化成Guava中的ListenableFuture，用到了JdkFutureAdapters这个适配器类。(以下源码来自guava-18.0.jar) 12345678910111213141516AsyncRestTemplate tp = new AsyncRestTemplate();org.springframework.util.concurrent.ListenableFuture&lt;ResponseEntity&lt;Object&gt;&gt; response = tp .getForEntity("http://blog.csdn.net/pistolove", Object.class);ListenableFuture&lt;ResponseEntity&lt;Object&gt;&gt; listenInPoolThread = JdkFutureAdapters.listenInPoolThread(response);Futures.addCallback(listenInPoolThread, new FutureCallback&lt;Object&gt;() &#123; @Override public void onSuccess(Object result) &#123; System.err.println(result.getClass()); System.err.printf("success", result); &#125; @Override public void onFailure(Throwable t) &#123; System.out.printf("failure"); &#125;&#125;); Futures的addCallback方法通过传入ListenableFuture和FutureCallback（一般情况FutureCallback实现为内部类）来实现回调机制。 1234//com.google.common.util.concurrent.Futurespublic static &lt;V&gt; void addCallback(ListenableFuture&lt;V&gt; future, FutureCallback&lt;? super V&gt; callback) &#123; addCallback(future, callback, directExecutor());&#125; 在addCallback方法中，我们发现多了一个 directExecutor()方法，这里的 directExecutor()方法返回的是一个枚举类型的线程池，这样做的目的是提高性能，而线程池中的execute方法实质执行的是所的传入参数Runnable 的run方法，可以把这里的线程池看作一个”架子”。 123456789101112//创建一个单实例的线程 接口需要显著的性能开销 提高性能public static Executor directExecutor() &#123; return DirectExecutor.INSTANCE;&#125;/** See &#123;@link #directExecutor&#125; for behavioral notes. */private enum DirectExecutor implements Executor &#123; INSTANCE; @Override public void execute(Runnable command) &#123; command.run(); &#125;&#125; 在具体的addCallback方法中，首先判断FutureCallback是否为空，然后创建一个线程，这个线程的run方法中会获取到一个value值，这里的value值即为http请求的结果，然后将value值传入FutureCallback的onSuccess方法，然后我们就可以在onSuccess方法中执行业务逻辑了。这个线程是如何执行的呢？继续往下看，发现调用了ListenableFuture的addListener方法，将刚才创建的线程和上一步创建的枚举线程池传入。 123456789101112131415161718192021222324252627282930313233//增加回调 public static &lt;V&gt; void addCallback(final ListenableFuture&lt;V&gt; future, final FutureCallback&lt;? super V&gt; callback, Executor executor) &#123; Preconditions.checkNotNull(callback); //每一个future进来都会创建一个独立的线程运行 Runnable callbackListener = new Runnable() &#123; @Override public void run() &#123; final V value; try &#123; // TODO(user): (Before Guava release), validate that this // is the thing for IE. //这里是真正阻塞的地方，直到获取到请求结果 value = getUninterruptibly(future); &#125; catch (ExecutionException e) &#123; callback.onFailure(e.getCause()); return; &#125; catch (RuntimeException e) &#123; callback.onFailure(e); return; &#125; catch (Error e) &#123; callback.onFailure(e); return; &#125; //调用callback的onSuccess方法返回结果 callback.onSuccess(value); &#125; &#125;; //增加监听，其中executor只提供了一个架子的线程池 future.addListener(callbackListener, executor); &#125; 在addListener方法中，将待执行的任务和枚举型线程池加入ExecutionList中，ExecutionList的本质是一个链表，将这些任务链接起来。具体可参考下方代码注释。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Override public void addListener(Runnable listener, Executor exec) &#123; //将监听任务和线程池加入到执行列表中 executionList.add(listener, exec); //This allows us to only start up a thread waiting on the delegate future when the first listener is added. //When a listener is first added, we run a task that will wait for the delegate to finish, and when it is done will run the listeners. //这允许我们启动一个线程来等待future当第一个监听器被加入的时候 //当第一个监听器被加入，我们将启动一个任务等待future完成，一旦当前的future完成后将会执行监听器 //判断是否有监听器加入 if (hasListeners.compareAndSet(false, true)) &#123; //如果当前的future完成则立即执行监听列表中的监听器,执行完成后返回 if (delegate.isDone()) &#123; // If the delegate is already done, run the execution list // immediately on the current thread. //执行监听列表中的监听任务 executionList.execute(); return; &#125; //如果当前的future没有完成，则启动线程池执行其中的任务，阻塞等待直到有一个future完成，然后执行监听器列表中的监听器 adapterExecutor.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; /* * Threads from our private pool are never interrupted. Threads * from a user-supplied executor might be, but... what can we do? * This is another reason to return a proper ListenableFuture * instead of using listenInPoolThread. */ getUninterruptibly(delegate); &#125; catch (Error e) &#123; throw e; &#125; catch (Throwable e) &#123; // ExecutionException / CancellationException / RuntimeException // The task is done, run the listeners. &#125; //执行链表中的任务 executionList.execute(); &#125; &#125;); &#125; &#125; &#125; 在ExecutionList的add方法中，判断是否执行完成，如果没有执行完成，则放入待执行的链表中并返回，否则调用executeListener方法执行任务，在executeListener方法中，我们发现执行的是线程池的execute方法，而execute方法实质的是调用了任务线程的run方法，这样最终会调用OnSuccess方法获取到执行结果。 12345678910111213141516171819202122232425//将任务放入ExecutionList中 public void add(Runnable runnable, Executor executor) &#123; // Fail fast on a null. We throw NPE here because the contract of // Executor states that it throws NPE on null listener, so we propagate // that contract up into the add method as well. Preconditions.checkNotNull(runnable, "Runnable was null."); Preconditions.checkNotNull(executor, "Executor was null."); // Lock while we check state. We must maintain the lock while adding the // new pair so that another thread can't run the list out from under us. // We only add to the list if we have not yet started execution. //判断是否执行完成，如果没有执行完成，则放入待执行的链表中 synchronized (this) &#123; if (!executed) &#123; runnables = new RunnableExecutorPair(runnable, executor, runnables); return; &#125; &#125; // Execute the runnable immediately. Because of scheduling this may end up // getting called before some of the previously added runnables, but we're // OK with that. If we want to change the contract to guarantee ordering // among runnables we'd have to modify the logic here to allow it. //执行监听 executeListener(runnable, executor); &#125; execute方法是执行任务链表中的任务，由于先加入的任务会依次排列在链表的末尾，所以需要将链表翻转。然后从链表头开始依次取出任务执行并放入枚举线程池中执行。 1234567891011121314151617181920212223242526272829303132333435363738394041//执行监听链表中的任务 public void execute() &#123; // Lock while we update our state so the add method above will finish adding // any listeners before we start to run them. //创建临时变量保存列表，并将成员变量置空让垃圾回收 RunnableExecutorPair list; synchronized (this) &#123; if (executed) &#123; return; &#125; executed = true; list = runnables; runnables = null; // allow GC to free listeners even if this stays around for a while. &#125; // If we succeeded then list holds all the runnables we to execute. The pairs in the stack are // in the opposite order from how they were added so we need to reverse the list to fulfill our // contract. // This is somewhat annoying, but turns out to be very fast in practice. Alternatively, we // could drop the contract on the method that enforces this queue like behavior since depending // on it is likely to be a bug anyway. // N.B. All writes to the list and the next pointers must have happened before the above // synchronized block, so we can iterate the list without the lock held here. //因为先加入的监听任务会在连边的末尾，所以需要将链表翻转 RunnableExecutorPair reversedList = null; while (list != null) &#123; RunnableExecutorPair tmp = list; list = list.next; tmp.next = reversedList; reversedList = tmp; &#125; //从链表头中依次取出监听任务执行 while (reversedList != null) &#123; executeListener(reversedList.runnable, reversedList.executor); reversedList = reversedList.next; &#125; &#125; 在上文中，可以发现每当对一个ListenerFuture增加回调时，都会创建一个线程，而这个线程的run方法中会获取一个value值，这个value值就是通过下面的getUninterruptibly方法获取到的，我们可以发现在方法中调用了while进行阻塞，一直等到future获取到结果，即发送的http请求获取到数据后才会终止并返回。可以看出，回调机制将获取结果中的阻塞分散开来，即使现在有100个线程在并发地发送http请求，那么也只是创建了100个独立的线程并行阻塞，那么运行的总时间则会是这100个线程中最长的时间，而不是100个线程的时间相加，这样就实现了异步非阻塞机制。 123456789101112131415161718//用while阻塞直到获取到结果 public static &lt;V&gt; V getUninterruptibly(Future&lt;V&gt; future) throws ExecutionException &#123; boolean interrupted = false; try &#123; while (true) &#123; try &#123; return future.get(); &#125; catch (InterruptedException e) &#123; interrupted = true; &#125; &#125; &#125; finally &#123; if (interrupted) &#123; Thread.currentThread().interrupt(); &#125; &#125; &#125; 这里实质上执行了线程的run方法，并进行阻塞。 12345678910111213//执行监听器，调用线程池的execute方法，这里线程池并没有提供额外的功能，只提供了执行架子，实际上执行的是监听任务runnable的run方法 //而在监听任务的run方法中，会阻塞获取请求结果，请求完成后回调，已达到异步执行的效果 private static void executeListener(Runnable runnable, Executor executor) &#123; try &#123; executor.execute(runnable); &#125; catch (RuntimeException e) &#123; // Log it and keep going, bad runnable and/or executor. Don't // punish the other runnables if we're given a bad one. We only // catch RuntimeException because we want Errors to propagate up. log.log(Level.SEVERE, "RuntimeException while executing runnable " + runnable + " with executor " + executor, e); &#125; &#125; 使用JdkFutureAdaoter适配Spring中的ListenableFuture达到异步调用的结果。在future.get方法中到底阻塞在什么地方呢？通过调试发现最后调用的是BasicFuture中的阻塞方法。详情见下方源码和中文注释，这里不累赘。 123456FutureAdapter: //这里的get方法会调用BasicFuture中的get方法进行阻塞，直到获取到结果 @Override public T get() throws InterruptedException, ExecutionException &#123; return adaptInternal(this.adaptee.get()); &#125; 12345678BasicFuture: //在这里会判断当前future是否执行完成，如果没有完成则会等待，一旦执行完成则返回结果。 public synchronized T get() throws InterruptedException, ExecutionException &#123; while (!this.completed) &#123; wait(); &#125; return getResult(); &#125; 123456789101112131415161718192021222324252627282930313233FutureAdapter: //这里通过判断状态是否success，如果success则返回成功，如果new, 则阻塞等待结果直到返回，然后改变状态。 @SuppressWarnings("unchecked") final T adaptInternal(S adapteeResult) throws ExecutionException &#123; synchronized (this.mutex) &#123; switch (this.state) &#123; case SUCCESS: return (T) this.result; case FAILURE: throw (ExecutionException) this.result; case NEW: try &#123; T adapted = adapt(adapteeResult); this.result = adapted; this.state = State.SUCCESS; return adapted; &#125; catch (ExecutionException ex) &#123; this.result = ex; this.state = State.FAILURE; throw ex; &#125; catch (Throwable ex) &#123; ExecutionException execEx = new ExecutionException(ex); this.result = execEx; this.state = State.FAILURE; throw execEx; &#125; default: throw new IllegalStateException(); &#125; &#125; &#125; 123456789101112131415//这个方法判断 public boolean completed(final T result) &#123; synchronized(this) &#123; if (this.completed) &#123; return false; &#125; this.completed = true; this.result = result; notifyAll(); &#125; if (this.callback != null) &#123; this.callback.completed(result); &#125; return true; &#125; 总结本文主要剖析了Futures.callback方法的源码，我们只需要一个ListenableFuture的实例，就可以使用该方法来实现回调机制。假设在我们的主线程中，有n个子方法要发送http请求，这时，我们可以创建n个ListenableFuture，对这n个ListenableFuture增加监听，这n个请求就是异步且非阻塞的，这样不但主线程不会阻塞，而且会大大减少总的响应时间。那Futures.callback是如何实现并发的呢？通过源码，我们发现，对于每一个ListenableFuture，都会创建一个独立的线程对其进行监听，也就是这n个ListenableFuture对应着n个独立的线程，而在每一个独立的线程中会各自调用Future.get方法阻塞。]]></content>
      <categories>
        <category>网络通信</category>
      </categories>
      <tags>
        <tag>async</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自己动手实现RPC框架（一）]]></title>
    <url>%2F2018%2F07%2F10%2Fepoch-rpc%2Fepoch-rpc-1%2F</url>
    <content type="text"><![CDATA[前言 RPC的概念看过很多，我的理解是：调用端获取到服务（网络方法）提供者的网络地址，并把方法调用的参数通过网络传递给提供者，提供者监听并获取到参数后，调用自己的方法，再把执行结果通过网络回传给调用端。这样站在调用端的角度看，就像是调用自己的本地方法一样，只不过慢一些而已。 RPC经典的架构图如下： RPC架构中可以认为有四个角色，消费者（Consumer），提供者（Provider），注册中心（Registry）和监控中心（Monitor）。以前在同一系统里的方法调用者因为网络的存在，变成了消费者，被调的方法成为了提供者。而所谓的注册中心，其实就是为了让消费者实时的去感知提供者的存在，去告诉消费者它对应的提供者的地址。监控中心，其实在整个过程中，它并不是一定要存在，只是它可以做统计，做一些数据分析，提供整个系统的可用性，健壮性。 好了，我们先简单分析一下 Registry / Consumer / Provider / Monitor 这四个角色的定义和每个角色如何各司其职，相互协作完成这整个过程的。 下面从两个方面进行分析，一是每个角色在网络的定位，二是每个角色所要完成的职责。 Registry注册中心简述： 注册中心可以有多个，都是无状态的，每个注册中心之间信息不交互 从网络的角度来说，它都是server端，它不需要主动地连接其他的任何实例，只需要像一个地主一样等待别人来连接 消费者随机选择注册中心集群中的任何实例建立长连接，提供者与注册中心中的每一个实例都建立长连接 职责(与其说职责，还不如说代码要实现的功能)： 接收服务提供者的服务注册信息，接收到信息之后，发送ACK信息给服务提供者，否则服务提供者重新发送注册信息 接收消费者的订阅信息，并把它订阅的结果返回给消费者 如果注册信息变更，会主动通知订阅变更信息的消费者，注册信息的变更包括服务提供者下线，服务被人工降级，或者服务提供者的地址变更 持久化一些服务信息，例如某些服务管理员审核过了，则该服务重新注册后则不需要再审核，再例如，某个服务负载均衡的策略被管理员设置为轮询，那么下次它在注册的时候，则就是轮询，而不是默认的负载策略 Provider提供者简述： 提供者是一个精神分裂的病人，它在网络上（可以更加明确地说是站在Netty的角度上）饰演两个角色： 它是客户端，需要去连接Registry，发送注册信息，它也需要去连接monitor端，去发送一些调用的统计信息 它也是服务端，需要作为server端等待Consumer去连接，连接成功后调用服务 职责： 将自己的信息，提供的接口信息编织成注册信息发送给registry端 能够动态去调自己的方法，可以通过反射，cglib等一些方法去调用自己提供的那些方法 提供服务降级等服务，如果当某些服务调用的失败率高于限定值的时候，可以有一个对应的mock方法，提供降级服务 限流服务，限流的方式有很多种，也有很多实现方式，最简单的就是控制调用次数，比如100w次，其实简单的就是控制单位时间的调用次数，防止业务洪流冲垮服务 统计活动，将一些调用信息统计好发送给Monitor端 Consumer消费者简述： 它也是有两个网络角色，不过并不是精神分裂，它都是作为网络的客户端存在，一它需要去连接registry去获取到订阅信息，二是它需要主动去连接provider端去调用服务 职责： 去向Registry端订阅服务，拿到registry端返回的结果，这个结果也就是provider的网络地址，先建立TCP的长连接，可能是多个地址，因为提供某个服务的可能有多个提供者 当开始系统主动调用该服务的时候，拿到刚才建立的连接的集合，根据某个方法，是随意还是轮询，获取到其中的一个连接，发送方法入参，等待响应 当注册中心发送某个服务的调用的负载策略发生变化过，发送信息给consumer，consumer需要做相应的变更 Monitor监控者简述： 这个与整个系统是没有任何直接的关系的，实现方式也是多样的，可以与上面一样建立长连接，接收每个角色统计的信息，然后展示给用户，可以使用MQ,使用消息队列，每个角色把自己统计的信息放到队列中，Monitor去消费这些信息，这样做的好处就是解耦，如果monitor宕了，不影响服务 大体的RPC的流程稍微理了一下，接下来我们就来一一去实现这些功能~]]></content>
      <categories>
        <category>分布式服务</category>
      </categories>
      <tags>
        <tag>rpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自己动手实现RPC框架（二）之项目结构]]></title>
    <url>%2F2018%2F07%2F10%2Fepoch-rpc%2Fepoch-rpc-2%2F</url>
    <content type="text"><![CDATA[前言 暂时还没法确定到底是什么样的包结构，等写完的时候再来填充这里。]]></content>
      <categories>
        <category>分布式服务</category>
      </categories>
      <tags>
        <tag>rpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式]]></title>
    <url>%2F2018%2F07%2F05%2Fjava-singleton%2F</url>
    <content type="text"><![CDATA[前言 在GoF的23种设计模式中，单例模式是比较简单的一种。然而，有时候越是简单的东西越容易出现问题。下面就单例设计模式详细的探讨一下。 所谓单例模式，简单来说，就是在整个应用中保证类只有一个实例存在。这个类的实例只提供了一个全局变量，用处相当广泛，比如保存全局数据，实现全局性的操作等。 最简单的实现首先，能够想到的最简单的实现是，把类的构造函数写成private的，从而保证别的类不能实例化此类，然后在类中提供一个静态的实例并能够返回给使用者。这样，使用者就可以通过这个引用使用到这个类的实例了。 123456789101112public class SingletonClass &#123; private static SingletonClass instance = new SingletonClass(); public static SingletonClass getInstance() &#123; return instance; &#125; private SingletonClass() &#123; &#125;&#125; 外部使用者如果需要使用SingletonClass的实例，只能通过getInstance()方法，并且它的构造方法是private的，这样就保证了只能有一个对象存在。 性能优化上面的代码虽然简单，但是有一个问题—-无论这个类是否被使用，都会创建一个instance对象。如果这个创建过程很耗时，比如需要连接10000次jdbc实例连接或者10000多个模版实例，并且这个类还并不一定会被使用，那么这个创建过程就是无用的。 为了解决这个问题，我们想到了新的解决方案： 123456789101112131415public class SingletonClass &#123; private static SingletonClass instance = null; public static SingletonClass getInstance() &#123; if (instance == null) &#123; instance = new SingletonClass(); &#125; return instance; &#125; private SingletonClass() &#123; &#125;&#125; 代码的变化有一处—-把instance初始化为null，直到第一次使用的时候通过判断是否为null来创建对象。 我们来想象一下这个过程。要使用SingletonClass，调用getInstance()方法。第一次的时候发现instance是null，然后就新建一个对象，返回出去；第二次再使用的时候，因为这个instance是static的，所以已经不是null了，因此不会再创建对象，直接将其返回。 这个过程就称为lazy loaded，也就是延迟加载—-直到使用的时候才进行加载。 同步上面的代码很清楚，也很简单。然而就像那句名言：”80%的错误都是由20%代码优化引起的”。单线程下，这段代码没有什么问题，可是如果是多线程，麻烦就来了。我们来分析一下： 线程1希望使用SingletonClass，调用getInstance()方法。因为是第一次调用，1就发现instance是null的，于是它开始创建实例，就在这个时候，CPU发生时间片切换(或者被抢夺执行)，线程2开始执行，它要使用SingletonClass，调用getInstance()方法，同样检测到instance是null—-注意，这是在1检测完之后切换的，也就是说1并没有来得及创建对象—-因此2开始创建。2创建完成后，cpu切换到1继续执行，因为它已经检测完了，所以1不会再检测一遍，它会直接创建对象。这样，线程1和2各自拥有一个SingletonClass的对象—-单例失败！解决的方法也很简单，那就是加锁： 12345678910111213141516public class SingletonClass &#123; private static SingletonClass instance = null; public synchronized static SingletonClass getInstance() &#123; if(instance == null) &#123; instance = new SingletonClass(); &#125; return instance; &#125; private SingletonClass() &#123; &#125;&#125; 又是性能问题上面的代码又是很清楚很简单的，然而，简单的东西往往不够理想。理想的东西往往不够简单，这就是生活。这段代码毫无疑问存在性能的问题—-synchronized修饰的同步块可是要比一般的代码段慢上几倍的！如果存在很多次getInstance()的调用，那性能问题就不得不考虑了！ 让我们来分析一下，究竟是整个方法都必须加锁，还是仅仅其中某一句加锁就足够了？我们为什么要加锁呢？分析一下出现lazy loaded的那种情形的原因。原因就是检测null的操作和创建对象的操作分离了。如果这两个操作能够原子地进行，那么单例就已经保证了。于是，我们开始修改代码： 1234567891011121314151617181920public class SingletonClass &#123; private static SingletonClass instance = null; public static SingletonClass getInstance() &#123; if (instance == null) &#123; synchronized (SingletonClass.class) &#123; if (instance == null) &#123; instance = new SingletonClass(); &#125; &#125; &#125; return instance; &#125; private SingletonClass() &#123; &#125;&#125; 还有问题吗？首先判断instance是不是为null，如果为null，加锁初始化；如果不为null，直接返回instance。 这就是double-checked locking设计实现单例模式。但是还有问题。 在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。那么Java内存模型规定了哪些东西呢，它定义了程序中变量的访问规则，往大一点说是定义了程序执行的次序。注意，为了获得较好的执行性能，Java内存模型并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序。也就是说，在java内存模型中，也会存在缓存一致性问题和指令重排序的问题。 下面来想一下，创建一个变量需要哪些步骤呢？一个是申请一块内存，调用构造方法进行初始化操作，另一个是分配一个指针指向这块内存。这两个操作谁在前谁在后呢？JMM规范并没有规定。（可能重排序）那么就存在这么一种情况，JVM是先开辟出一块内存，然后把指针指向这块内存，最后调用构造方法进行初始化。 线程1开始创建SingletonClass的实例，此时线程B调用了getInstance()方法，首先判断instance是否为null。按照我们上面所说的内存模型，1已经把instance指向了那块内存，只是还没有调用构造方法，因此2检测到instance不为null，于是直接把instance返回了—-问题出现了，尽管instance不为null，但它并没有构造完成，就像一套房子已经给了你钥匙，但你并不能住进去，因为里面还是毛坯房。此时，如果2在1将instance构造完成之前就是用了这个实例，程序就会出现错误了！ 最终解决方案在JDK 5之后，Java使用了新的内存模型。volatile关键字有了明确的语义—-在JDK1.5之前，volatile是个关键字，但是并没有明确的规定其用途—-被volatile修饰的写变量不能和之前的读写代码调整，读变量不能和之后的读写代码调整！因此，只要我们简单的把instance加上volatile关键字就可以了。 1234567891011121314151617181920public class SingletonClass &#123; private volatile static SingletonClass instance = null; public static SingletonClass getInstance() &#123; if (instance == null) &#123; synchronized (SingletonClass.class) &#123; if(instance == null) &#123; instance = new SingletonClass(); &#125; &#125; &#125; return instance; &#125; private SingletonClass() &#123; &#125;&#125;]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git常用操作记录]]></title>
    <url>%2F2018%2F07%2F05%2Ftools%2Fgit-operation%2F</url>
    <content type="text"><![CDATA[获取当前分支名 1git symbolic-ref --short -q HEAD 将本地仓库和github仓库关联起来 12git remote add github git@github.com:liningrui/study-rpc.gitgit pull 再查看所有分支就可以看到github远端分支的信息了 1git branch -av 删除github远端的分支 1git push github :travis 这样就删除了travis分支 创建orphan分支，名为source 1git checkout --orphan source 注：如果不提交东西，这个分支实际上没有创建 查看某个指定文件的提交历史记录 1git log -p filePath 这样就先显示指定文件的每一次提交及修改信息（diff），但是不能显示文件改名前的修改，要注意第一次提交是不是改文件名 查看某一个分支创建的时间 1git reflog show --date=iso branch 最下面的应该就是该分支的创建时间 修改提交历史 1、找到要修改的commit id及其前一个commit id 1git rebase -i --before-commit-id 弹出来的一堆以 pick 开头的 commit id 和 commit message 的行，将第一行（也允许修改多行）的 pick 修改为 edit，然后保存退出vim，git 会在标记的 commit 停下来，然后我们可以做相应的修改，再执行 12git commit -a --amendgit rebase --continue 这时 git 会打印 rebasing(progress/total)，中间很有可能会产生冲突，解决好冲突后执行 12git add filegit rebase --continue 一直往下走，遇到冲突就重复这一步，直到走完全部的提交，这样就实现了修改历史。]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自己动手实现RPC框架]]></title>
    <url>%2F2018%2F07%2F05%2Fepoch-rpc%2Fepoch-rpc-0%2F</url>
    <content type="text"><![CDATA[前言 RPC的文章看了不少，但是始终感觉似懂非懂，古人说的”纸上得来终觉浅，绝知此事要躬行”还是很有道理的。所以我希望通过一个真正的项目，来加深对RPC的认识。 这应该会是一个系列文章，至少在我动笔的这一刻是有非常强烈的意愿完成它的。 主要参考CSDN博客一起写RPC框架开篇说明 2018-09-03续，果不其然，还是因为诸多事情耽搁了。 今天又看了一些RPC的文章，发现黄勇老师的轻量级分布式 RPC 框架写的非常清晰易懂。于是整理出一些关键点出来，方便自己实现。 编写RPC框架的步骤如下： 编写服务接口 编写服务接口的实现类 配置服务端 启动服务器并发布服务 实现服务注册 实现 RPC 服务器 配置客户端 实现服务发现 实现 RPC 代理 发送 RPC 请求 注册中心的职责核心的职责如下： 服务提供者向其发送它提供的服务的一些基本信息，并完成注册 服务消费者订阅服务 服务提供者下线的时候，实时通知服务消费者某个服务下线 除此之外，还有一些锦上添花但在真实的业务中必须得有的功能，比如： 服务审核，这是服务治理最最简单的操作了，因为某个服务提供者上线之后，都是需要审核的，如果不审核，可能会造成很多不必要的麻烦，有可能有些开发小新，不小心把开发环境的服务向线上服务注册，如果不审核，直接通过的话，就会造成线上接口调用线下服务的尴尬局面 负载策略的记录，比如默认是随机加权策略，如果管理者希望改成加权轮询的策略，需要通知服务消费者，访问策略的改变 手动改变某个服务的访问权重，比如某个服务默认负重是50，（最大100）的时候，但是此时这个服务实例所在的机器压力不大的时候，而其他该服务实例压力很大的时候，可以适当的增加该服务的访问权重，所以我们可以在注册中心修改它的负重，然后通知服务消费者，这样就可以动态的修改负重了 一些持久化的操作，因为注册中心是无状态的，假如某个注册中心实例重启之后，以前的一些审核信息，修改的访问策略信息就会消失，这样就会需要用户重新一一审核，这是很麻烦的，所以需要将这些信息落地，持久化到硬盘，然后每次重启注册中心实例的时候，去读取这些信息 基于ZooKeeper的注册中心节点树结构如下： 计划（暂定） 2018-07-08 架构设计，功能（细节）设计 2018-07-15 网络传输模型，序列化部分 2018-07-22 服务端框架 2018-07-29 客户端框架 2018-08-06 负载均衡 2018-08-13 服务降级 2018-08-20 测试与优化]]></content>
      <categories>
        <category>分布式服务</category>
      </categories>
      <tags>
        <tag>rpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac 下安装 jekyll]]></title>
    <url>%2F2018%2F07%2F04%2Fmac-install-jekyll%2F</url>
    <content type="text"><![CDATA[Mac 下安装 jekyll1sudo gem install jekyll 输入密码，但还是会提示没有写权限 12ERROR: While executing gem ... (Gem::FilePermissionError) You don&apos;t have write permissions for the /usr/bin directory. 原因是 Apple在OS X El Capitan中全面启用了名为System Integrity Protection (SIP)的系统完整性保护技术。受此影响，大部分系统文件即使在root用户下也无法直接进行修改。 升级ruby（推荐） 安装RVM1curl -L get.rvm.io | bash -s stable 出现异常 12345678910111213141516171819gpg: Signature made 一 7/ 2 03:41:26 2018 CSTgpg: using RSA key 62C9E5F4DA300D94AC36166BE206C29FBF04FF17gpg: Can&apos;t check signature: No public keyWarning, RVM 1.26.0 introduces signed releases and automated check of signatures when GPG software found. Assuming you trust Michal Papis import the mpapis public key (downloading the signatures).GPG signature verification failed for &apos;/Users/liningrui/.rvm/archives/rvm-1.29.4.tgz&apos; - &apos;https://github.com/rvm/rvm/releases/download/1.29.4/1.29.4.tar.gz.asc&apos;! Try to install GPG v2 and then fetch the public key: gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3or if it fails: command curl -sSL https://rvm.io/mpapis.asc | gpg2 --import -the key can be compared with: https://rvm.io/mpapis.asc https://keybase.io/mpapisNOTE: GPG version 2.1.17 have a bug which cause failures during fetching keys from remote server. Please downgrade or upgrade to newer version (if available) or use the second method described above. 你是因为我本地安装了gpg，但是却没有它的公钥，所以我们需要先接受公钥到本地。 1gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 然后再执行上述命令，就应该Ok了。 1234567891011121314151617gpg: Signature made 一 7/ 2 03:41:26 2018 CSTgpg: using RSA key 62C9E5F4DA300D94AC36166BE206C29FBF04FF17gpg: Good signature from &quot;Michal Papis (RVM signing) &lt;mpapis@gmail.com&gt;&quot; [unknown]gpg: aka &quot;Michal Papis &lt;michal.papis@toptal.com&gt;&quot; [unknown]gpg: aka &quot;[jpeg image of size 5015]&quot; [unknown]gpg: WARNING: This key is not certified with a trusted signature!gpg: There is no indication that the signature belongs to the owner.Primary key fingerprint: 409B 6B17 96C2 7546 2A17 0311 3804 BB82 D39D C0E3 Subkey fingerprint: 62C9 E5F4 DA30 0D94 AC36 166B E206 C29F BF04 FF17GPG verified &apos;/Users/liningrui/.rvm/archives/rvm-1.29.4.tgz&apos;Installing RVM to /Users/liningrui/.rvm/ Adding rvm PATH line to /Users/liningrui/.profile /Users/liningrui/.mkshrc /Users/liningrui/.bashrc /Users/liningrui/.zshrc. Adding rvm loading line to /Users/liningrui/.profile /Users/liningrui/.bash_profile /Users/liningrui/.zlogin.Installation of RVM in /Users/liningrui/.rvm/ is almost complete: * To start using RVM you need to run `source /Users/liningrui/.rvm/scripts/rvm` in all your open shell windows, in rare cases you need to reopen all shell windows. 它提示说要使用RVM需要将rvm添加到环境变量中。 12source /Users/liningrui/.rvm/scripts/rvmrvm -v 列出所有可用的ruby版本 1rvm list known 安装最新版本的ruby（以2.5.1为例） 1rvm install 2.5.1 安装jekyll1gem install jekyll 安装完成后，cd到项目根目录，使用以下命令即可运行jekyll环境，通过 localhost:4000 即可访问。 1jekyll serve 提示1Dependency Error: Yikes! It looks like you don&apos;t have jekyll-paginate or one of its dependencies installed. In order to use Jekyll as currently configured, you&apos;ll need to install this gem. The full error message from Ruby is: &apos;cannot load such file -- jekyll-paginate&apos; If you run into trouble, you can find helpful resources at https://jekyllrb.com/help/! 安装即可 1gem install jekyll-paginate 接下来就可以开始github pages之路了～ 参考： https://www.cnblogs.com/kaiye/archive/2013/04/24/3039345.html https://blog.csdn.net/andanlan/article/details/50061775]]></content>
  </entry>
  <entry>
    <title><![CDATA[阻塞、非阻塞、同步、异步的区别]]></title>
    <url>%2F2018%2F07%2F04%2Fnetwork-io%2F</url>
    <content type="text"><![CDATA[我认为同步、异步、阻塞、非阻塞，是分3个层次的： CPU层次； 线程层次； 程序员感知层次。 这几个概念之所以容易混淆，是因为没有分清楚是在哪个层次进行讨论。 CPU层次在 CPU 层次，或者说操作系统进行 IO 和任务调度的层次，现代操作系统通常使用异步非阻塞方式进行 IO（有少部分 IO 可能会使用同步非阻塞轮询），即发出 IO 请求之后，并不等待 IO 操作完成，而是继续执行下面的指令（非阻塞），IO 操作和 CPU 指令互不干扰（异步），最后通过中断的方式来通知 IO 操作完成结果。 线程层次在线程层次，或者说操作系统调度单元的层次，操作系统为了减轻程序员的思考负担，将底层的异步非阻塞的IO方式进行封装，把相关系统调用（如read，write等）以同步的方式展现出来。然而，同步阻塞的IO会使线程挂起，同步非阻塞的IO会消耗CPU资源在轮询上。为了解决这一问题，就有3种思路： 多线程（同步阻塞）； IO 多路复用（select，poll，epoll）（同步非阻塞，严格地来讲，是把阻塞点改变了位置）； 直接暴露出异步的 IO 接口，如 kernel-aio 和 IOCP（异步非阻塞）。 程序员感知层次在Linux中，上面提到的第2种思路用得比较广泛，也是比较理想的解决方案。然而，直接使用select之类的接口，依然比较复杂，所以各种库和框架百花齐放，都试图对IO多路复用进行封装。此时，库和框架提供的API又可以选择是以同步的方式还是异步的方式来展现。如python的asyncio库中，就通过协程，提供了同步阻塞式的API；如node.js中，就通过回调函数，提供了异步非阻塞式的API。 总结因此，我们在讨论同步、异步、阻塞、非阻塞时，必须先明确是在哪个层次进行讨论。比如node.js，我们可以说她在程序员感知层次提供了异步非阻塞的API，也可以说在Linux下，她在线程层次以同步非阻塞的epoll来实现。]]></content>
      <categories>
        <category>网络通信</category>
      </categories>
      <tags>
        <tag>NIO，IO</tag>
      </tags>
  </entry>
</search>
